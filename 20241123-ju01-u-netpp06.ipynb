{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install segmentation-models-pytorch\n",
    "# !pip install pytorch_msssim\n",
    "# !pip install lightning\n",
    "# !pip install lightning[extra]\n",
    "# !pip install tensorboard\n",
    "# !pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install torch pillow torchvision opencv-python numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import cv2\n",
    "\n",
    "import zipfile\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "0\n",
      "NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "11.8\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())  # True인지 확인\n",
    "print(torch.cuda.device_count())  # 사용 가능한 GPU 개수 확인\n",
    "print(torch.cuda.current_device())  # 현재 사용 중인 GPU 인덱스 확인\n",
    "print(torch.cuda.get_device_name(0))  # 사용 중인 GPU 이름 확인\n",
    "print(torch.version.cuda)  # PyTorch가 사용하는 CUDA 버전 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29603\n",
      "29603\n"
     ]
    }
   ],
   "source": [
    "origin_dir = 'data/train_gt'\n",
    "damage_dir = 'data/train_input'\n",
    "\n",
    "print(len(os.listdir(origin_dir)))\n",
    "print(len(os.listdir(damage_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'EPOCHS':2,\n",
    "    'LEARNING_RATE':3e-4,\n",
    "    # 'BATCH_SIZE':16,\n",
    "    'BATCH_SIZE':16,\n",
    "    'SEED':42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True  # 결정론적 연산 보장\n",
    "    torch.backends.cudnn.benchmark = False     # 성능 최적화 대신 일관성 우선\n",
    "\n",
    "seed_everything(CFG['SEED'])  # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random: 0.6394267984578837\n",
      "NumPy Random: [0.37454012]\n",
      "PyTorch Random: tensor([0.8823])\n"
     ]
    }
   ],
   "source": [
    "seed_everything(42)\n",
    "\n",
    "# 테스트 난수\n",
    "print(\"Random:\", random.random())\n",
    "print(\"NumPy Random:\", np.random.rand(1))\n",
    "print(\"PyTorch Random:\", torch.rand(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_image(damage_img_path, origin_img_path):\n",
    "    # OpenCV로 이미지 읽기 (NumPy 배열로 읽음)\n",
    "    color_image = cv2.imread(origin_img_path)\n",
    "    gray_image = cv2.imread(damage_img_path, cv2.IMREAD_GRAYSCALE)  # 흑백 이미지로 읽기\n",
    "    \n",
    "    # 색상 이미지를 흑백으로 변환 (PIL로 변환 후 NumPy로 변환)\n",
    "    color_image_gray = cv2.cvtColor(color_image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # 두 이미지의 차이 계산\n",
    "    difference = cv2.absdiff(color_image_gray, gray_image)\n",
    "    \n",
    "    # 차이 값을 임계값으로 처리하여 이진화 이미지 생성\n",
    "    _, binary_difference = cv2.threshold(difference, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # 마스크 생성\n",
    "    mask = binary_difference > 0  # 차이가 있는 부분을 마스크로 설정\n",
    "    mask = Image.fromarray(mask.astype(np.uint8) * 255)  # 마스크 이미지를 PIL 형식으로 변환\n",
    "\n",
    "    return {\n",
    "        'image_gray_masked': Image.fromarray(gray_image),  # 손상된 이미지를 PIL 이미지로 반환\n",
    "        'mask': transforms.ToTensor()(mask)  # 마스크를 텐서로 변환하여 사용\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, damage_dir, origin_dir, transform=None, use_masks=True):\n",
    "        self.damage_dir = damage_dir\n",
    "        self.origin_dir = origin_dir\n",
    "        self.transform = transform\n",
    "        self.use_masks = use_masks\n",
    "        self.damage_files = sorted(os.listdir(damage_dir), key=lambda x: x.lower())\n",
    "        self.origin_files = sorted(os.listdir(origin_dir), key=lambda x: x.lower())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.damage_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        damage_img_name = self.damage_files[idx]\n",
    "        origin_img_name = self.origin_files[idx]\n",
    "\n",
    "        damage_img_path = os.path.join(self.damage_dir, damage_img_name)\n",
    "        origin_img_path = os.path.join(self.origin_dir, origin_img_name)\n",
    "\n",
    "        damage_img = Image.open(damage_img_path).convert(\"RGB\")\n",
    "        origin_img = Image.open(origin_img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.use_masks:\n",
    "            input_data = get_input_image(damage_img_path, origin_img_path)\n",
    "            mask = input_data['mask']\n",
    "            # `mask`가 이미 텐서인지 확인하고 변환 처리\n",
    "            if not isinstance(mask, torch.Tensor):\n",
    "                mask = transforms.ToTensor()(mask)\n",
    "        else:\n",
    "            mask = torch.zeros((1, damage_img.size[1], damage_img.size[0]))\n",
    "\n",
    "        if self.transform:\n",
    "            damage_img = self.transform(damage_img)\n",
    "            origin_img = self.transform(origin_img)\n",
    "\n",
    "        return {'A': damage_img, 'B': origin_img, 'mask': mask}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo:\n",
    "# 1. val_score\n",
    "# 2. get_input_image(image) 테스트해서 원본 (손상된 흑백)과 마스크(512*512로 나오되 배경은 다 0, 마스크부분은 회색)\n",
    "# 3. loss 잘못됨 (아마 컬러를 흑백으로 바꾸면 걔를 정답으로 써도 될듯)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_step의 loss계산부분 변경 -> 원본 컬러사진을 흑백으로 변환해서 비고하는 것으로\n",
    "# validation_step의 score 반환부분 변경 -> 학습완료시 최적의 모델 저장이 되지않는 부분으로 실제 학습에 적용이 되는지 확인필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zqrc0\\anaconda3\\envs\\new_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from segmentation_models_pytorch import UnetPlusPlus\n",
    "import torch.nn.functional as F\n",
    "import lightning as L\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def get_histogram_similarity(true_np, pred_np, color_space=cv2.COLOR_RGB2HSV):\n",
    "    # BGR 이미지를 HSV로 변환\n",
    "    true_hsv = cv2.cvtColor(true_np.astype(np.uint8), color_space)\n",
    "    pred_hsv = cv2.cvtColor(pred_np.astype(np.uint8), color_space)\n",
    "    \n",
    "    # 히스토그램 계산 및 비교\n",
    "    hist_true = cv2.calcHist([true_hsv], [0], None, [180], [0, 180])\n",
    "    hist_pred = cv2.calcHist([pred_hsv], [0], None, [180], [0, 180])\n",
    "    \n",
    "    hist_true = cv2.normalize(hist_true, hist_true).flatten()\n",
    "    hist_pred = cv2.normalize(hist_pred, hist_pred).flatten()\n",
    "    \n",
    "    similarity = cv2.compareHist(hist_true, hist_pred, cv2.HISTCMP_CORREL)\n",
    "    return similarity\n",
    "\n",
    "def get_masked_ssim_score(true_np, pred_np, mask_np):\n",
    "    # true_np: (height, width, channels)\n",
    "    # pred_np: (height, width, channels)\n",
    "    # mask_np: (height, width) or (1, height, width)\n",
    "    \n",
    "    # mask_np 차원 변환\n",
    "    if mask_np.ndim == 3 and mask_np.shape[0] == 1:\n",
    "        mask_np = mask_np.squeeze(0)  # (1, height, width) -> (height, width)\n",
    "    elif mask_np.ndim == 3 and mask_np.shape[-1] == 1:\n",
    "        mask_np = mask_np.squeeze(-1)  # (height, width, 1) -> (height, width)\n",
    "    \n",
    "    # true_np와 mask_np의 크기가 맞지 않으면 리사이즈\n",
    "    if true_np.shape[:2] != mask_np.shape:\n",
    "        mask_np = cv2.resize(mask_np, (true_np.shape[1], true_np.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    # 마스크가 적용된 부분만 추출\n",
    "    true_masked = true_np[mask_np > 0]\n",
    "    pred_masked = pred_np[mask_np > 0]\n",
    "\n",
    "    if true_masked.size == 0 or pred_masked.size == 0:\n",
    "        return 0  # 마스크된 부분이 없으면 SSIM을 0으로 처리\n",
    "\n",
    "    # SSIM 계산\n",
    "    ssim_value = ssim(\n",
    "        true_masked, pred_masked, data_range=pred_masked.max() - pred_masked.min(), channel_axis=-1\n",
    "    )\n",
    "    return ssim_value\n",
    "\n",
    "\n",
    "def ssim_score(true, pred):\n",
    "    true_np = true.permute(0, 2, 3, 1).cpu().numpy()\n",
    "    pred_np = pred.permute(0, 2, 3, 1).cpu().numpy()\n",
    "\n",
    "    print(f\"true_np shape: {true_np.shape}\")\n",
    "    print(f\"pred_np shape: {pred_np.shape}\")\n",
    "    print(f\"mask_np shape: {mask_np.shape}\")\n",
    "\n",
    "    scores = [\n",
    "        ssim(t, p, channel_axis=-1, data_range=p.max() - p.min())\n",
    "        for t, p in zip(true_np, pred_np)\n",
    "    ]\n",
    "    return np.mean(scores)\n",
    "\n",
    "def get_ssim_score(true_np, pred_np):\n",
    "    \"\"\"\n",
    "    두 이미지를 비교하여 SSIM(Structural Similarity Index Measure)을 계산합니다.\n",
    "    NaN 값을 방지하기 위해 추가적인 조건을 추가합니다.\n",
    "\n",
    "    Args:\n",
    "        true_np (numpy.ndarray): Ground truth 이미지 (H, W, C 형태)\n",
    "        pred_np (numpy.ndarray): 예측 이미지 (H, W, C 형태)\n",
    "\n",
    "    Returns:\n",
    "        float: SSIM 값 (NaN 발생 시 0으로 설정)\n",
    "    \"\"\"\n",
    "    ssim_value = ssim(\n",
    "        true_np, pred_np, channel_axis=-1, data_range=pred_np.max() - pred_np.min()\n",
    "    )\n",
    "    if np.isnan(ssim_value):\n",
    "        ssim_value = 0  # NaN 발생 시 SSIM 값을 0으로 설정\n",
    "    return ssim_value\n",
    "\n",
    "# LitIRModel 클래스 정의\n",
    "class LitIRModel(L.LightningModule):\n",
    "    def __init__(self, model_1, model_2, image_mean=0.5, image_std=0.5):\n",
    "        super().__init__()\n",
    "        self.model_1 = model_1\n",
    "        self.model_2 = model_2\n",
    "        self.image_mean = image_mean\n",
    "        self.image_std = image_std\n",
    "        self.validation_outputs = []\n",
    "\n",
    "    def forward(self, images_gray_masked):\n",
    "        images_gray_restored = self.model_1(images_gray_masked) + images_gray_masked\n",
    "        images_restored = self.model_2(images_gray_restored)\n",
    "        return images_gray_restored, images_restored\n",
    "\n",
    "    def unnormalize(self, output, round=False):\n",
    "        image_restored = ((output * self.image_std + self.image_mean) * 255).clamp(0, 255)\n",
    "        if round:\n",
    "            image_restored = torch.round(image_restored)\n",
    "        return image_restored\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.AdamW(self.parameters(), lr=1e-5)\n",
    "        return opt\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images_gray_masked = batch['A']  # 손상된 이미지 정답: mask가 채워진 흑백사진\n",
    "        images_gt = batch['B']  # Ground Truth 이미지\n",
    "        masks = batch['masks']  # 마스크 (현재 사용 여부에 따라 다름)\n",
    "\n",
    "        # Ground Truth 이미지를 흑백으로 변환\n",
    "        images_gray_masked = torch.mean(images_gt, dim=1, keepdim=True)  # RGB 평균을 계산하여 흑백으로 변환\n",
    "    \n",
    "        # 모델에 입력\n",
    "        images_gray_restored, images_restored = self(images_gray_masked)\n",
    "    \n",
    "        # 손실 계산\n",
    "        loss_pixel_gray = (\n",
    "            F.l1_loss(images_gray_masked, images_gray_restored, reduction='mean') * 0.5 +\n",
    "            F.mse_loss(images_gray_masked, images_gray_restored, reduction='mean') * 0.5\n",
    "        )\n",
    "        loss_pixel = (\n",
    "            F.l1_loss(images_gt, images_restored, reduction='mean') * 0.5 +\n",
    "            F.mse_loss(images_gt, images_restored, reduction='mean') * 0.5\n",
    "        )\n",
    "        loss = loss_pixel_gray * 0.5 + loss_pixel * 0.5\n",
    "\n",
    "        # 로깅 (Batch와 손실 값 출력)\n",
    "        print(f\"Batch {batch_idx}, Loss: {loss.item()}\")\n",
    "        \n",
    "        # 로그 기록\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Extract data from batch\n",
    "        images_gray_masked = batch['A']\n",
    "        images_gt = batch['B']\n",
    "        masks = batch['masks']\n",
    "\n",
    "        # Forward pass\n",
    "        images_gray_restored, images_restored = self(images_gray_masked)\n",
    "\n",
    "        # Ground Truth와 복원된 이미지 크기 맞추기\n",
    "        if images_restored.shape != images_gt.shape:\n",
    "            images_restored = torch.nn.functional.interpolate(\n",
    "                images_restored, size=images_gt.shape[-2:], mode=\"bilinear\", align_corners=False\n",
    "            )\n",
    "\n",
    "        # NumPy 변환\n",
    "        images_restored_np = images_restored.detach().cpu().permute(0, 2, 3, 1).float().numpy().astype(np.uint8)\n",
    "        images_gt_np = images_gt.detach().cpu().permute(0, 2, 3, 1).float().numpy().astype(np.uint8)\n",
    "        masks_np = masks.detach().cpu().numpy()\n",
    "\n",
    "        # Metric 계산\n",
    "        total_ssim_score = 0\n",
    "        masked_ssim_score = 0\n",
    "        hist_sim_score = 0\n",
    "        for image_gt_np, image_restored_np, mask_np in zip(images_gt_np, images_restored_np, masks_np):\n",
    "            total_ssim_score += get_ssim_score(image_gt_np, image_restored_np) / len(images_gt)\n",
    "            masked_ssim_score += get_masked_ssim_score(image_gt_np, image_restored_np, mask_np) / len(images_gt)\n",
    "            hist_sim_score += get_histogram_similarity(image_gt_np, image_restored_np) / len(images_gt)\n",
    "\n",
    "        # 최종 점수\n",
    "        score = total_ssim_score * 0.2 + masked_ssim_score * 0.4 + hist_sim_score * 0.4\n",
    "\n",
    "        # 배치별 검증 결과 출력\n",
    "        print(f\"Validation Batch {batch_idx}, Total SSIM: {total_ssim_score:.4f}, \"\n",
    "              f\"Masked SSIM: {masked_ssim_score:.4f}, Histogram Similarity: {hist_sim_score:.4f}\")\n",
    "\n",
    "        # 로깅\n",
    "        self.log(\"val_score\", score, on_step=False, on_epoch=True)\n",
    "        self.log(\"val_total_ssim_score\", total_ssim_score, on_step=False, on_epoch=True)\n",
    "        self.log(\"val_masked_ssim_score\", masked_ssim_score, on_step=False, on_epoch=True)\n",
    "        self.log(\"val_hist_sim_score\", hist_sim_score, on_step=False, on_epoch=True)\n",
    "\n",
    "        return {'val_score': score}\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        total_ssim_score = 0\n",
    "        masked_ssim_score = 0\n",
    "        hist_sim_score = 0\n",
    "\n",
    "        for output in self.validation_outputs:\n",
    "            images_gt = output[\"images_gt\"]\n",
    "            images_restored = output[\"images_restored\"]\n",
    "            masks = output[\"masks\"]\n",
    "\n",
    "            masks_np = masks.detach().cpu().numpy()\n",
    "            images_gt_np = images_gt.detach().cpu().permute(0, 2, 3, 1).float().numpy().astype(np.uint8)\n",
    "            images_restored_np = images_restored.detach().cpu().permute(0, 2, 3, 1).float().numpy().astype(np.uint8)\n",
    "\n",
    "            for image_gt_np, image_restored_np, mask_np in zip(images_gt_np, images_restored_np, masks_np):\n",
    "                total_ssim_score += get_ssim_score(image_gt_np, image_restored_np) / len(images_gt_np)\n",
    "                masked_ssim_score += get_masked_ssim_score(image_gt_np, image_restored_np, mask_np) / len(images_gt_np)\n",
    "                hist_sim_score += get_histogram_similarity(image_gt_np, image_restored_np, cv2.COLOR_RGB2HSV) / len(images_gt_np)\n",
    "\n",
    "        score = total_ssim_score * 0.2 + masked_ssim_score * 0.4 + hist_sim_score * 0.4\n",
    "\n",
    "        self.log(f\"val_score\", score, on_step=False, on_epoch=True)\n",
    "        self.log(f\"val_total_ssim_score\", total_ssim_score, on_step=False, on_epoch=True)\n",
    "        self.log(f\"val_masked_ssim_score\", masked_ssim_score, on_step=False, on_epoch=True)\n",
    "        self.log(f\"val_hist_sim_score\", hist_sim_score, on_step=False, on_epoch=True)\n",
    "\n",
    "        self.validation_outputs = []\n",
    "\n",
    "# 모델 초기화\n",
    "# 첫 번째 모델: Gray Mask Restoration\n",
    "model_1 = UnetPlusPlus(\n",
    "    encoder_name=\"efficientnet-b4\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=1\n",
    ")\n",
    "\n",
    "# 두 번째 모델: Gray → Color\n",
    "model_2 = UnetPlusPlus(\n",
    "    encoder_name=\"efficientnet-b4\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=3\n",
    ")\n",
    "\n",
    "# LitIRModel 초기화\n",
    "lit_ir_model = LitIRModel(model_1=model_1, model_2=model_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zqrc0\\anaconda3\\envs\\new_env\\lib\\site-packages\\lightning\\fabric\\connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4070 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "c:\\Users\\zqrc0\\anaconda3\\envs\\new_env\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:654: Checkpoint directory D:\\OneDrive\\human\\project\\image\\saved_models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type         | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | model_1 | UnetPlusPlus | 20.8 M | train\n",
      "1 | model_2 | UnetPlusPlus | 20.8 M | train\n",
      "-------------------------------------------------\n",
      "41.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "41.6 M    Total params\n",
      "166.506   Total estimated model params size (MB)\n",
      "1274      Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zqrc0\\anaconda3\\envs\\new_env\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:419: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import lightning as L\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# 경로 설정\n",
    "origin_dir = 'data/train_gt'\n",
    "damage_dir = 'data/train_input'\n",
    "test_dir = 'data/test_input'\n",
    "\n",
    "# 데이터 전처리 설정\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),  # 모든 이미지를 512x512으로 리사이즈\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "# 데이터셋 생성\n",
    "dataset = CustomDataset(damage_dir=damage_dir, origin_dir=origin_dir, transform=transform)\n",
    "\n",
    "# 데이터셋 분할\n",
    "validation_ratio = 0.2\n",
    "train_size = int((1 - validation_ratio) * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "training_dataset, validation_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "class CollateFn:\n",
    "    def __init__(self, mode='train'):\n",
    "        self.mode = mode\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        A = torch.stack([item['A'] for item in batch])\n",
    "        B = torch.stack([item['B'] for item in batch])\n",
    "        masks = torch.stack([item['mask'] for item in batch]) if 'mask' in batch[0] else torch.zeros_like(A)\n",
    "\n",
    "        if self.mode in ['train', 'valid']:\n",
    "            return {'A': A, 'B': B, 'masks': masks}\n",
    "        elif self.mode == 'test':\n",
    "            return {'A': A}\n",
    "\n",
    "# CollateFn 정의\n",
    "train_collate_fn = CollateFn(mode='train')\n",
    "validation_collate_fn = CollateFn(mode='valid')\n",
    "\n",
    "# DataLoader 설정\n",
    "train_dataloader = DataLoader(\n",
    "    training_dataset,\n",
    "    batch_size=CFG['BATCH_SIZE'],\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    collate_fn=train_collate_fn\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size=CFG['BATCH_SIZE'],\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    collate_fn=validation_collate_fn\n",
    ")\n",
    "\n",
    "# 모델 저장 디렉토리 생성\n",
    "model_save_dir = \"./saved_models\"\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "# Trainer 설정 및 학습 시작\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=CFG['EPOCHS'],\n",
    "    precision=16,\n",
    "    accelerator='gpu',\n",
    "    devices=1,\n",
    "    log_every_n_steps=10,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(\n",
    "            monitor='val_score',\n",
    "            mode='max',\n",
    "            save_top_k=10 ,\n",
    "            dirpath=model_save_dir,\n",
    "            filename='best_model-{epoch:02d}-{val_score:.4f}'\n",
    "        ),\n",
    "        EarlyStopping(monitor='val_score', mode='max', patience=3)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 학습 시작\n",
    "trainer.fit(lit_ir_model, train_dataloader, validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 데이터 전처리\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "\n",
    "# 테스트 데이터셋 생성\n",
    "test_dataset = CustomDataset(damage_dir=test_dir, origin_dir=test_dir, transform=transform, use_masks=False)\n",
    "\n",
    "# 테스트 DataLoader 생성\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=CFG['BATCH_SIZE'],  # 적절히 설정\n",
    "    shuffle=False,\n",
    "    num_workers=2  # 경고를 피하기 위해 적절히 설정\n",
    ")\n",
    "\n",
    "# 모델 초기화\n",
    "model = LitIRModel.load_from_checkpoint(\n",
    "    checkpoint_path='saved_models/best_model-epoch=01-val_score=0.0000.ckpt',\n",
    "    model_1=model_1,\n",
    "    model_2=model_2\n",
    ")\n",
    "\n",
    "# 모델을 평가 모드로 설정\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# 테스트 데이터로 예측 실행\n",
    "output_dir = \"output/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "model.eval()  # 모델을 평가 모드로 설정\n",
    "with torch.no_grad():\n",
    "    for idx, batch in enumerate(test_dataloader):\n",
    "        # 입력 데이터 준비 (RGB -> Grayscale 변환)\n",
    "        inputs = torch.mean(batch['A'], dim=1, keepdim=True).to(device)  # [N, 1, H, W]\n",
    "        \n",
    "        # 모델 예측\n",
    "        gray_restored, color_restored = model(inputs)  # 모델 예측\n",
    "\n",
    "        # 예측된 이미지를 저장\n",
    "        for i, result in enumerate(color_restored):\n",
    "            result_img = (result.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)  # [C, H, W] -> [H, W, C]\n",
    "            output_path = os.path.join(output_dir, f\"output_{idx}_{i}.png\")\n",
    "            plt.imsave(output_path, result_img)\n",
    "            print(f\"Saved: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6075062,
     "sourceId": 9891694,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6078662,
     "sourceId": 9896364,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6111024,
     "sourceId": 9939640,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
