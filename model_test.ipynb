{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install segmentation-models-pytorch\n",
    "# !pip install pytorch_msssim\n",
    "# !pip install lightning\n",
    "# !pip install lightning[extra]\n",
    "# !pip install tensorboard\n",
    "# !pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'log_trace_structured_event' from 'torch._utils_internal' (c:\\Users\\zqrc0\\anaconda3\\envs\\tensorflow-env\\lib\\site-packages\\torch\\_utils_internal.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msegmentation_models_pytorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UnetPlusPlus\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mL\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskimage\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m structural_similarity \u001b[38;5;28;01mas\u001b[39;00m ssim\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelCheckpoint, EarlyStopping\n",
      "File \u001b[1;32mc:\\Users\\zqrc0\\anaconda3\\envs\\tensorflow-env\\lib\\site-packages\\lightning\\__init__.py:18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__about__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: E402, F403\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__version__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version \u001b[38;5;28;01mas\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Fabric  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mseed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m seed_everything  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callback  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zqrc0\\anaconda3\\envs\\tensorflow-env\\lib\\site-packages\\lightning\\fabric\\__init__.py:30\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Setting this variable will force `torch.cuda.is_available()` and `torch.cuda.device_count()`\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# to use an NVML-based implementation that doesn't poison forks.\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/83973\u001b[39;00m\n\u001b[0;32m     27\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTORCH_NVML_BASED_CUDA_CHECK\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Fabric  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mseed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m seed_everything  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m disable_possible_user_warnings  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zqrc0\\anaconda3\\envs\\tensorflow-env\\lib\\site-packages\\lightning\\fabric\\fabric.py:46\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maccelerators\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maccelerator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Accelerator\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnector\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _PLUGIN_INPUT, _PRECISION_INPUT, _Connector, _is_using_cli\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloggers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Logger\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Precision  \u001b[38;5;66;03m# avoid circular imports: # isort: split\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstrategies\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     49\u001b[0m     DataParallelStrategy,\n\u001b[0;32m     50\u001b[0m     DeepSpeedStrategy,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m     XLAStrategy,\n\u001b[0;32m     55\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\zqrc0\\anaconda3\\envs\\tensorflow-env\\lib\\site-packages\\lightning\\fabric\\loggers\\__init__.py:15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloggers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcsv_logs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CSVLogger  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloggers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Logger  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloggers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorboard\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TensorBoardLogger  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zqrc0\\anaconda3\\envs\\tensorflow-env\\lib\\site-packages\\lightning\\fabric\\loggers\\tensorboard.py:30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrank_zero\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rank_zero_only, rank_zero_warn\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _PATH\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _unwrap_objects\n\u001b[0;32m     32\u001b[0m _TENSORBOARD_AVAILABLE \u001b[38;5;241m=\u001b[39m RequirementCache(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorboard\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m _TENSORBOARDX_AVAILABLE \u001b[38;5;241m=\u001b[39m RequirementCache(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorboardX\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\zqrc0\\anaconda3\\envs\\tensorflow-env\\lib\\site-packages\\lightning\\fabric\\wrappers.py:38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn \u001b[38;5;28;01mas\u001b[39;00m nn\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OptimizedModule\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _IncompatibleKeys\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optimizer\n",
      "File \u001b[1;32mc:\\Users\\zqrc0\\anaconda3\\envs\\tensorflow-env\\lib\\site-packages\\torch\\_dynamo\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_frame, eval_frame, resume_execution\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m callback_handler, on_compile_end, on_compile_start\n",
      "File \u001b[1;32mc:\\Users\\zqrc0\\anaconda3\\envs\\tensorflow-env\\lib\\site-packages\\torch\\_dynamo\\convert_frame.py:28\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mweakref\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReferenceType\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_logging\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mguards\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GlobalStateGuard\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_compile_pg\n",
      "File \u001b[1;32mc:\\Users\\zqrc0\\anaconda3\\envs\\tensorflow-env\\lib\\site-packages\\torch\\_logging\\__init__.py:7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Top level logging module for torch logging\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Design doc: https://docs.google.com/document/d/1ZRfTWKa8eaPq1AxaiHrq4ASTPouzzlPiuquSBEJYwS8/edit#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Simple setup for onboarding (see above doc for more detail):\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 1. register any top-level log qualified name for your module in torch._logging._registrations (see there for examples)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 2. register any artifacts (<artifact_name> below) in torch._logging._registrations\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#   a. call getArtifactLogger(__name__, <artifact_name>) at your logging site instead of the standard logger to log your artifact\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_logging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_registrations\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     _init_logs,\n\u001b[0;32m     11\u001b[0m     DEFAULT_LOGGING,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     warning_once,\n\u001b[0;32m     17\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\zqrc0\\anaconda3\\envs\\tensorflow-env\\lib\\site-packages\\torch\\_logging\\_registrations.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# flake8: noqa: B950\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register_artifact, register_log\n\u001b[0;32m      5\u001b[0m DYNAMIC \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.fx.experimental.symbolic_shapes\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.fx.experimental.sym_node\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.fx.experimental.recording\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m ]\n\u001b[0;32m     10\u001b[0m DISTRIBUTED \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.distributed\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch._dynamo.backends.distributed\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.nn.parallel.distributed\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     14\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\zqrc0\\anaconda3\\envs\\tensorflow-env\\lib\\site-packages\\torch\\_logging\\_internal.py:19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mweakref\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WeakSet\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_logging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstructured\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m log_trace_structured_event\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_traceback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CapturedTraceback\n\u001b[0;32m     23\u001b[0m log \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'log_trace_structured_event' from 'torch._utils_internal' (c:\\Users\\zqrc0\\anaconda3\\envs\\tensorflow-env\\lib\\site-packages\\torch\\_utils_internal.py)"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split, Dataset, DataLoader\n",
    "import cv2\n",
    "\n",
    "from segmentation_models_pytorch import UnetPlusPlus\n",
    "import torch.nn.functional as F\n",
    "import lightning as L\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "CFG = {\n",
    "    'EPOCHS':2,\n",
    "    'LEARNING_RATE':3e-4,\n",
    "    # 'BATCH_SIZE':16,\n",
    "    'BATCH_SIZE':16,\n",
    "    'SEED':42\n",
    "}\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True  # 결정론적 연산 보장\n",
    "    torch.backends.cudnn.benchmark = False     # 성능 최적화 대신 일관성 우선\n",
    "\n",
    "seed_everything(CFG['SEED'])  # Seed 고정\n",
    "\n",
    "def get_input_image(damage_img_path, origin_img_path):\n",
    "    # OpenCV로 이미지 읽기 (NumPy 배열로 읽음)\n",
    "    color_image = cv2.imread(origin_img_path)\n",
    "    gray_image = cv2.imread(damage_img_path, cv2.IMREAD_GRAYSCALE)  # 흑백 이미지로 읽기\n",
    "    \n",
    "    # 색상 이미지를 흑백으로 변환 (PIL로 변환 후 NumPy로 변환)\n",
    "    color_image_gray = cv2.cvtColor(color_image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # 두 이미지의 차이 계산\n",
    "    difference = cv2.absdiff(color_image_gray, gray_image)\n",
    "    \n",
    "    # 차이 값을 임계값으로 처리하여 이진화 이미지 생성\n",
    "    _, binary_difference = cv2.threshold(difference, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # 마스크 생성\n",
    "    mask = binary_difference > 0  # 차이가 있는 부분을 마스크로 설정\n",
    "    mask = Image.fromarray(mask.astype(np.uint8) * 255)  # 마스크 이미지를 PIL 형식으로 변환\n",
    "\n",
    "    return {\n",
    "        'image_gray_masked': Image.fromarray(gray_image),  # 손상된 이미지를 PIL 이미지로 반환\n",
    "        'mask': transforms.ToTensor()(mask)  # 마스크를 텐서로 변환하여 사용\n",
    "    }\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, damage_dir, origin_dir, transform=None, use_masks=True):\n",
    "        self.damage_dir = damage_dir\n",
    "        self.origin_dir = origin_dir\n",
    "        self.transform = transform\n",
    "        self.use_masks = use_masks\n",
    "        self.damage_files = sorted(os.listdir(damage_dir), key=lambda x: x.lower())\n",
    "        self.origin_files = sorted(os.listdir(origin_dir), key=lambda x: x.lower())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.damage_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        damage_img_name = self.damage_files[idx]\n",
    "        origin_img_name = self.origin_files[idx]\n",
    "\n",
    "        damage_img_path = os.path.join(self.damage_dir, damage_img_name)\n",
    "        origin_img_path = os.path.join(self.origin_dir, origin_img_name)\n",
    "\n",
    "        damage_img = Image.open(damage_img_path).convert(\"RGB\")\n",
    "        origin_img = Image.open(origin_img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.use_masks:\n",
    "            tmp_damage_img = damage_img.convert(\"L\")\n",
    "            input_data = get_input_image(damage_img_path, origin_img_path)\n",
    "            mask = transforms.ToTensor()(input_data['mask'])\n",
    "        else:\n",
    "            mask = torch.zeros((1, damage_img.size[1], damage_img.size[0]))\n",
    "\n",
    "        if self.transform:\n",
    "            damage_img = self.transform(damage_img)\n",
    "            origin_img = self.transform(origin_img)\n",
    "\n",
    "        return {'A': damage_img, 'B': origin_img, 'mask': mask}\n",
    "\n",
    "\n",
    "\n",
    "def get_histogram_similarity(true_np, pred_np, color_space=cv2.COLOR_RGB2HSV):\n",
    "    # BGR 이미지를 HSV로 변환\n",
    "    true_hsv = cv2.cvtColor(true_np.astype(np.uint8), color_space)\n",
    "    pred_hsv = cv2.cvtColor(pred_np.astype(np.uint8), color_space)\n",
    "    \n",
    "    # 히스토그램 계산 및 비교\n",
    "    hist_true = cv2.calcHist([true_hsv], [0], None, [180], [0, 180])\n",
    "    hist_pred = cv2.calcHist([pred_hsv], [0], None, [180], [0, 180])\n",
    "    \n",
    "    hist_true = cv2.normalize(hist_true, hist_true).flatten()\n",
    "    hist_pred = cv2.normalize(hist_pred, hist_pred).flatten()\n",
    "    \n",
    "    similarity = cv2.compareHist(hist_true, hist_pred, cv2.HISTCMP_CORREL)\n",
    "    return similarity\n",
    "\n",
    "def get_masked_ssim_score(true_np, pred_np, mask_np):\n",
    "    # true_np: (height, width, channels)\n",
    "    # pred_np: (height, width, channels)\n",
    "    # mask_np: (height, width) or (1, height, width)\n",
    "    \n",
    "    # mask_np 차원 변환\n",
    "    if mask_np.ndim == 3 and mask_np.shape[0] == 1:\n",
    "        mask_np = mask_np.squeeze(0)  # (1, height, width) -> (height, width)\n",
    "    elif mask_np.ndim == 3 and mask_np.shape[-1] == 1:\n",
    "        mask_np = mask_np.squeeze(-1)  # (height, width, 1) -> (height, width)\n",
    "    \n",
    "    # true_np와 mask_np의 크기가 맞지 않으면 리사이즈\n",
    "    if true_np.shape[:2] != mask_np.shape:\n",
    "        mask_np = cv2.resize(mask_np, (true_np.shape[1], true_np.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    # 마스크가 적용된 부분만 추출\n",
    "    true_masked = true_np[mask_np > 0]\n",
    "    pred_masked = pred_np[mask_np > 0]\n",
    "\n",
    "    if true_masked.size == 0 or pred_masked.size == 0:\n",
    "        return 0  # 마스크된 부분이 없으면 SSIM을 0으로 처리\n",
    "\n",
    "    # SSIM 계산\n",
    "    ssim_value = ssim(\n",
    "        true_masked, pred_masked, data_range=pred_masked.max() - pred_masked.min(), channel_axis=-1\n",
    "    )\n",
    "    return ssim_value\n",
    "\n",
    "\n",
    "def ssim_score(true, pred):\n",
    "    true_np = true.permute(0, 2, 3, 1).cpu().numpy()\n",
    "    pred_np = pred.permute(0, 2, 3, 1).cpu().numpy()\n",
    "\n",
    "    print(f\"true_np shape: {true_np.shape}\")\n",
    "    print(f\"pred_np shape: {pred_np.shape}\")\n",
    "    print(f\"mask_np shape: {mask_np.shape}\")\n",
    "\n",
    "    scores = [\n",
    "        ssim(t, p, channel_axis=-1, data_range=p.max() - p.min())\n",
    "        for t, p in zip(true_np, pred_np)\n",
    "    ]\n",
    "    return np.mean(scores)\n",
    "\n",
    "def get_ssim_score(true_np, pred_np):\n",
    "    \"\"\"\n",
    "    두 이미지를 비교하여 SSIM(Structural Similarity Index Measure)을 계산합니다.\n",
    "    NaN 값을 방지하기 위해 추가적인 조건을 추가합니다.\n",
    "\n",
    "    Args:\n",
    "        true_np (numpy.ndarray): Ground truth 이미지 (H, W, C 형태)\n",
    "        pred_np (numpy.ndarray): 예측 이미지 (H, W, C 형태)\n",
    "\n",
    "    Returns:\n",
    "        float: SSIM 값 (NaN 발생 시 0으로 설정)\n",
    "    \"\"\"\n",
    "    ssim_value = ssim(\n",
    "        true_np, pred_np, channel_axis=-1, data_range=pred_np.max() - pred_np.min()\n",
    "    )\n",
    "    if np.isnan(ssim_value):\n",
    "        ssim_value = 0  # NaN 발생 시 SSIM 값을 0으로 설정\n",
    "    return ssim_value\n",
    "\n",
    "# LitIRModel 클래스 정의\n",
    "class LitIRModel(L.LightningModule):\n",
    "    def __init__(self, model_1, model_2, image_mean=0.5, image_std=0.5):\n",
    "        super().__init__()\n",
    "        self.model_1 = model_1\n",
    "        self.model_2 = model_2\n",
    "        self.image_mean = image_mean\n",
    "        self.image_std = image_std\n",
    "        self.validation_outputs = []\n",
    "\n",
    "        self.val_score = 0\n",
    "\n",
    "    def forward(self, images_gray_masked):\n",
    "        images_gray_restored = self.model_1(images_gray_masked) + images_gray_masked\n",
    "        images_restored = self.model_2(images_gray_restored)\n",
    "        return images_gray_restored, images_restored\n",
    "\n",
    "    def unnormalize(self, output, round=False):\n",
    "        image_restored = ((output * self.image_std + self.image_mean) * 255).clamp(0, 255)\n",
    "        if round:\n",
    "            image_restored = torch.round(image_restored)\n",
    "        return image_restored\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.AdamW(self.parameters(), lr=1e-5)\n",
    "        return opt\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images_gray_masked = batch['A']  # 손상된 이미지 정답: mask가 채워진 흑백사진\n",
    "        images_gt = batch['B']  # Ground Truth 이미지\n",
    "        masks = batch['masks']  # 마스크 (현재 사용 여부에 따라 다름)\n",
    "\n",
    "        # 모델에 입력\n",
    "        images_gray_restored, images_restored = self(images_gray_masked)\n",
    "\n",
    "        # 손실 계산\n",
    "        loss_pixel_gray = ( # F.l1_loss(정답:구멍채워진흑백, 즉 컬러를 흑백으로 바꾸면됨, 예측:구멍채우려고 노력하는model1, reductio\n",
    "            F.l1_loss(images_gray_masked, images_gray_restored, reduction='mean') * 0.5 +\n",
    "            F.mse_loss(images_gray_masked, images_gray_restored, reduction='mean') * 0.5\n",
    "        )\n",
    "        loss_pixel = (\n",
    "            F.l1_loss(images_gt, images_restored, reduction='mean') * 0.5 +\n",
    "            F.mse_loss(images_gt, images_restored, reduction='mean') * 0.5\n",
    "        )\n",
    "        loss = loss_pixel_gray * 0.5 + loss_pixel * 0.5\n",
    "\n",
    "        # 로깅 (Batch와 손실 값 출력)\n",
    "        print(f\"Batch {batch_idx}, Loss: {loss.item()}\")\n",
    "        \n",
    "        # 로그 기록\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Extract data from batch\n",
    "        images_gray_masked = batch['A']\n",
    "        images_gt = batch['B']\n",
    "        masks = batch['masks']\n",
    "\n",
    "        # Forward pass\n",
    "        images_gray_restored, images_restored = self(images_gray_masked)\n",
    "\n",
    "        # Ground Truth와 복원된 이미지 크기 맞추기\n",
    "        if images_restored.shape != images_gt.shape:\n",
    "            images_restored = torch.nn.functional.interpolate(\n",
    "                images_restored, size=images_gt.shape[-2:], mode=\"bilinear\", align_corners=False\n",
    "            )\n",
    "\n",
    "        # NumPy 변환\n",
    "        images_restored_np = images_restored.detach().cpu().permute(0, 2, 3, 1).float().numpy().astype(np.uint8)\n",
    "        images_gt_np = images_gt.detach().cpu().permute(0, 2, 3, 1).float().numpy().astype(np.uint8)\n",
    "        masks_np = masks.detach().cpu().numpy()\n",
    "\n",
    "        # Metric 계산\n",
    "        total_ssim_score = 0\n",
    "        masked_ssim_score = 0\n",
    "        hist_sim_score = 0\n",
    "        for image_gt_np, image_restored_np, mask_np in zip(images_gt_np, images_restored_np, masks_np):\n",
    "            total_ssim_score += get_ssim_score(image_gt_np, image_restored_np) / len(images_gt)\n",
    "            masked_ssim_score += get_masked_ssim_score(image_gt_np, image_restored_np, mask_np) / len(images_gt)\n",
    "            hist_sim_score += get_histogram_similarity(image_gt_np, image_restored_np) / len(images_gt)\n",
    "\n",
    "        # 최종 점수\n",
    "        score = total_ssim_score * 0.2 + masked_ssim_score * 0.4 + hist_sim_score * 0.4\n",
    "        val_score += score\n",
    "\n",
    "        # 배치별 검증 결과 출력\n",
    "        print(f\"Validation Batch {batch_idx}, SSIM Score: {total_ssim_score:.4f}\")\n",
    "\n",
    "        # 로깅\n",
    "        self.log(\"val_score\", score, on_step=False, on_epoch=True)\n",
    "        self.log(\"val_total_ssim_score\", total_ssim_score, on_step=False, on_epoch=True)\n",
    "        self.log(\"val_masked_ssim_score\", masked_ssim_score, on_step=False, on_epoch=True)\n",
    "        self.log(\"val_hist_sim_score\", hist_sim_score, on_step=False, on_epoch=True)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        total_ssim_score = 0\n",
    "        masked_ssim_score = 0\n",
    "        hist_sim_score = 0\n",
    "\n",
    "        for output in self.validation_outputs:\n",
    "            images_gt = output[\"images_gt\"]\n",
    "            images_restored = output[\"images_restored\"]\n",
    "            masks = output[\"masks\"]\n",
    "\n",
    "            masks_np = masks.detach().cpu().numpy()\n",
    "            images_gt_np = images_gt.detach().cpu().permute(0, 2, 3, 1).float().numpy().astype(np.uint8)\n",
    "            images_restored_np = images_restored.detach().cpu().permute(0, 2, 3, 1).float().numpy().astype(np.uint8)\n",
    "\n",
    "            for image_gt_np, image_restored_np, mask_np in zip(images_gt_np, images_restored_np, masks_np):\n",
    "                total_ssim_score += get_ssim_score(image_gt_np, image_restored_np) / len(images_gt_np)\n",
    "                masked_ssim_score += get_masked_ssim_score(image_gt_np, image_restored_np, mask_np) / len(images_gt_np)\n",
    "                hist_sim_score += get_histogram_similarity(image_gt_np, image_restored_np, cv2.COLOR_RGB2HSV) / len(images_gt_np)\n",
    "\n",
    "        score = total_ssim_score * 0.2 + masked_ssim_score * 0.4 + hist_sim_score * 0.4\n",
    "\n",
    "        self.log(f\"val_score\", score, on_step=False, on_epoch=True)\n",
    "        self.log(f\"val_total_ssim_score\", total_ssim_score, on_step=False, on_epoch=True)\n",
    "        self.log(f\"val_masked_ssim_score\", masked_ssim_score, on_step=False, on_epoch=True)\n",
    "        self.log(f\"val_hist_sim_score\", hist_sim_score, on_step=False, on_epoch=True)\n",
    "\n",
    "        self.validation_outputs = []\n",
    "\n",
    "# 모델 초기화\n",
    "# 첫 번째 모델: Gray Mask Restoration\n",
    "model_1 = UnetPlusPlus(\n",
    "    encoder_name=\"efficientnet-b4\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=1\n",
    ")\n",
    "\n",
    "# 두 번째 모델: Gray → Color\n",
    "model_2 = UnetPlusPlus(\n",
    "    encoder_name=\"efficientnet-b4\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=3\n",
    ")\n",
    "\n",
    "# LitIRModel 초기화\n",
    "lit_ir_model = LitIRModel(model_1=model_1, model_2=model_2)\n",
    "\n",
    "\n",
    "\n",
    "# 경로 설정\n",
    "origin_dir = 'data/train_gt'\n",
    "damage_dir = 'data/train_input'\n",
    "test_dir = 'data/test_input'\n",
    "\n",
    "# 데이터 전처리 설정\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # 모든 이미지를 256x256으로 리사이즈\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "# 데이터셋 생성\n",
    "dataset = CustomDataset(damage_dir=damage_dir, origin_dir=origin_dir, transform=transform)\n",
    "\n",
    "# 데이터셋 분할\n",
    "validation_ratio = 0.2\n",
    "train_size = int((1 - validation_ratio) * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "training_dataset, validation_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "class CollateFn:\n",
    "    def __init__(self, mode='train'):\n",
    "        self.mode = mode\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        A = torch.stack([item['A'] for item in batch])\n",
    "        B = torch.stack([item['B'] for item in batch])\n",
    "        masks = torch.stack([item['mask'] for item in batch]) if 'mask' in batch[0] else torch.zeros_like(A)\n",
    "\n",
    "        if self.mode in ['train', 'valid']:\n",
    "            return {'A': A, 'B': B, 'masks': masks}\n",
    "        elif self.mode == 'test':\n",
    "            return {'A': A}\n",
    "\n",
    "# CollateFn 정의\n",
    "train_collate_fn = CollateFn(mode='train')\n",
    "validation_collate_fn = CollateFn(mode='valid')\n",
    "\n",
    "# DataLoader 설정\n",
    "train_dataloader = DataLoader(\n",
    "    training_dataset,\n",
    "    batch_size=CFG['BATCH_SIZE'],\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    collate_fn=train_collate_fn\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size=CFG['BATCH_SIZE'],\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    collate_fn=validation_collate_fn\n",
    ")\n",
    "\n",
    "# 모델 저장 디렉토리 생성\n",
    "model_save_dir = \"./saved_models\"\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "# Trainer 설정 및 학습 시작\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=CFG['EPOCHS'],\n",
    "    precision=16,\n",
    "    accelerator='gpu',\n",
    "    devices=1,\n",
    "    log_every_n_steps=10,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(\n",
    "            monitor='val_score',\n",
    "            mode='max',\n",
    "            save_top_k=1,\n",
    "            dirpath=model_save_dir,\n",
    "            filename='best_model-{epoch:02d}-{val_score:.4f}'\n",
    "        ),\n",
    "        EarlyStopping(monitor='val_score', mode='max', patience=3)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 학습 시작\n",
    "trainer.fit(lit_ir_model, train_dataloader, validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
