{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T11:24:26.629975Z",
     "iopub.status.busy": "2024-11-14T11:24:26.629172Z",
     "iopub.status.idle": "2024-11-14T11:24:38.130162Z",
     "shell.execute_reply": "2024-11-14T11:24:38.129043Z",
     "shell.execute_reply.started": "2024-11-14T11:24:26.629932Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch_msssim\n",
      "  Downloading pytorch_msssim-1.0.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\zqrc0\\anaconda3\\envs\\tensorflow-env\\lib\\site-packages (from pytorch_msssim) (2.0.0+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\zqrc0\\anaconda3\\envs\\tensorflow-env\\lib\\site-packages (from torch->pytorch_msssim) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\zqrc0\\anaconda3\\envs\\tensorflow-env\\lib\\site-packages (from torch->pytorch_msssim) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\zqrc0\\anaconda3\\envs\\tensorflow-env\\lib\\site-packages (from torch->pytorch_msssim) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\zqrc0\\anaconda3\\envs\\tensorflow-env\\lib\\site-packages (from torch->pytorch_msssim) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\zqrc0\\anaconda3\\envs\\tensorflow-env\\lib\\site-packages (from torch->pytorch_msssim) (3.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\zqrc0\\anaconda3\\envs\\tensorflow-env\\lib\\site-packages (from jinja2->torch->pytorch_msssim) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\zqrc0\\anaconda3\\envs\\tensorflow-env\\lib\\site-packages (from sympy->torch->pytorch_msssim) (1.3.0)\n",
      "Downloading pytorch_msssim-1.0.0-py3-none-any.whl (7.7 kB)\n",
      "Installing collected packages: pytorch_msssim\n",
      "Successfully installed pytorch_msssim-1.0.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install segmentation-models-pytorch\n",
    "# !pip install pytorch_msssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T09:18:37.670994Z",
     "iopub.status.busy": "2024-11-14T09:18:37.670658Z",
     "iopub.status.idle": "2024-11-14T09:18:42.640463Z",
     "shell.execute_reply": "2024-11-14T09:18:42.639488Z",
     "shell.execute_reply.started": "2024-11-14T09:18:37.670958Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import cv2\n",
    "\n",
    "import zipfile\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T09:18:42.642256Z",
     "iopub.status.busy": "2024-11-14T09:18:42.641727Z",
     "iopub.status.idle": "2024-11-14T09:18:42.646988Z",
     "shell.execute_reply": "2024-11-14T09:18:42.646120Z",
     "shell.execute_reply.started": "2024-11-14T09:18:42.642210Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'EPOCHS':2,\n",
    "    'LEARNING_RATE':3e-4,\n",
    "    # 'BATCH_SIZE':16,\n",
    "    'BATCH_SIZE':64,\n",
    "    'SEED':42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T09:18:42.650513Z",
     "iopub.status.busy": "2024-11-14T09:18:42.649547Z",
     "iopub.status.idle": "2024-11-14T09:18:42.666019Z",
     "shell.execute_reply": "2024-11-14T09:18:42.665174Z",
     "shell.execute_reply.started": "2024-11-14T09:18:42.650471Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T09:18:42.667573Z",
     "iopub.status.busy": "2024-11-14T09:18:42.667226Z",
     "iopub.status.idle": "2024-11-14T09:18:42.677566Z",
     "shell.execute_reply": "2024-11-14T09:18:42.676594Z",
     "shell.execute_reply.started": "2024-11-14T09:18:42.667532Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#저장된 이미지 쌍을 동시에 로드 \n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, damage_dir, origin_dir, transform=None):\n",
    "        self.damage_dir = damage_dir\n",
    "        self.origin_dir = origin_dir\n",
    "        self.transform = transform\n",
    "        self.damage_files = sorted(os.listdir(damage_dir))\n",
    "        self.origin_files = sorted(os.listdir(origin_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.damage_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        damage_img_name = self.damage_files[idx]\n",
    "        origin_img_name = self.origin_files[idx]\n",
    "\n",
    "        damage_img_path = os.path.join(self.damage_dir, damage_img_name)\n",
    "        origin_img_path = os.path.join(self.origin_dir, origin_img_name)\n",
    "\n",
    "        damage_img = Image.open(damage_img_path).convert(\"RGB\")\n",
    "        origin_img = Image.open(origin_img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            damage_img = self.transform(damage_img)\n",
    "            origin_img = self.transform(origin_img)\n",
    "\n",
    "        return {'A': damage_img, 'B': origin_img}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T09:18:42.679214Z",
     "iopub.status.busy": "2024-11-14T09:18:42.678753Z",
     "iopub.status.idle": "2024-11-14T09:18:43.438842Z",
     "shell.execute_reply": "2024-11-14T09:18:43.437922Z",
     "shell.execute_reply.started": "2024-11-14T09:18:42.679172Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# 경로 설정\n",
    "origin_dir = 'data/train_gt'  # 원본 이미지 폴더 경로\n",
    "damage_dir = 'data/train_input'  # 손상된 이미지 폴더 경로\n",
    "test_dir = 'data/test_input'     # test 이미지 폴더 경로\n",
    "\n",
    "# 데이터 전처리 설정\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "# 전체 데이터셋 생성\n",
    "dataset = CustomDataset(damage_dir=damage_dir, origin_dir=origin_dir, transform=transform)\n",
    "\n",
    "# 데이터셋을 학습과 검증으로 나누기 (예: 80% 학습, 20% 검증)\n",
    "validation_ratio = 0.2\n",
    "train_size = int((1 - validation_ratio) * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "training_dataset, validation_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# 학습 및 검증 DataLoader 설정\n",
    "train_dataloader = DataLoader(training_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True, num_workers=1)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T09:18:43.440344Z",
     "iopub.status.busy": "2024-11-14T09:18:43.440037Z",
     "iopub.status.idle": "2024-11-14T09:18:43.450364Z",
     "shell.execute_reply": "2024-11-14T09:18:43.449487Z",
     "shell.execute_reply.started": "2024-11-14T09:18:43.440313Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# PatchGAN 기반의 Discriminator\n",
    "class PatchGANDiscriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(PatchGANDiscriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, normalization=True):\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, kernel_size=4, stride=2, padding=1)]\n",
    "            if normalization:\n",
    "                layers.append(nn.BatchNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return nn.Sequential(*layers)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            discriminator_block(in_channels * 2, 64, normalization=False),\n",
    "            discriminator_block(64, 128),\n",
    "            discriminator_block(128, 256),\n",
    "            discriminator_block(256, 512),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, img_A, img_B):\n",
    "        img_input = torch.cat((img_A, img_B), 1)\n",
    "        return self.model(img_input)\n",
    "\n",
    "# 가중치 초기화 함수\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm2d') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T09:18:43.451957Z",
     "iopub.status.busy": "2024-11-14T09:18:43.451376Z",
     "iopub.status.idle": "2024-11-14T09:18:50.986698Z",
     "shell.execute_reply": "2024-11-14T09:18:50.985907Z",
     "shell.execute_reply.started": "2024-11-14T09:18:43.451913Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from segmentation_models_pytorch import UnetPlusPlus\n",
    "\n",
    "# EfficientNet-B4를 백본으로 사용\n",
    "UNetPP = UnetPlusPlus(\n",
    "    encoder_name=\"efficientnet-b4\",  # pretrained on ImageNet\n",
    "    encoder_weights=\"imagenet\",     \n",
    "    in_channels=3,\n",
    "    classes=3\n",
    ").to(device)\n",
    "\n",
    "# generator로 UNetPP를 사용하도록 설정\n",
    "generator = UNetPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 45\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, CFG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEPOCHS\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     44\u001b[0m     generator\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# 학습 모드\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     46\u001b[0m         real_A \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     47\u001b[0m         real_B \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\zqrc0\\anaconda3\\envs\\tensorflow-env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:442\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zqrc0\\anaconda3\\envs\\tensorflow-env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:388\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zqrc0\\anaconda3\\envs\\tensorflow-env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1043\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1036\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1043\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1045\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mc:\\Users\\zqrc0\\anaconda3\\envs\\tensorflow-env\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zqrc0\\anaconda3\\envs\\tensorflow-env\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zqrc0\\anaconda3\\envs\\tensorflow-env\\lib\\multiprocessing\\context.py:327\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zqrc0\\anaconda3\\envs\\tensorflow-env\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 93\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\zqrc0\\anaconda3\\envs\\tensorflow-env\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from IPython.display import FileLink\n",
    "from pytorch_msssim import ssim\n",
    "\n",
    "# 모델 저장을 위한 디렉토리 생성\n",
    "model_save_dir = \"./saved_models\"\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "log_file = os.path.join(model_save_dir, \"training_log.txt\")\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "lambda_pixel = 100  # 픽셀 손실에 대한 가중치\n",
    "\n",
    "# PatchGAN Discriminator 초기화\n",
    "discriminator = PatchGANDiscriminator().to(device)\n",
    "\n",
    "# 손실 함수 및 옵티마이저 설정\n",
    "criterion_GAN = nn.MSELoss()\n",
    "criterion_pixelwise = nn.L1Loss()\n",
    "\n",
    "# 색상 히스토그램 유사도 함수\n",
    "def histogram_similarity(pred, target, bins=256):\n",
    "    pred_hist = cv2.calcHist([pred.cpu().numpy()], [0], None, [bins], [0, 256])\n",
    "    target_hist = cv2.calcHist([target.cpu().numpy()], [0], None, [bins], [0, 256])\n",
    "    hist_similarity = cv2.compareHist(pred_hist, target_hist, cv2.HISTCMP_CORREL)\n",
    "    return hist_similarity\n",
    "\n",
    "# 손실 함수\n",
    "def combined_loss(fake_B, real_B):\n",
    "    loss_GAN = criterion_GAN(fake_B, real_B)\n",
    "    loss_pixel = criterion_pixelwise(fake_B, real_B)\n",
    "    loss_ssim = 1 - ssim(fake_B, real_B, data_range=1.0)\n",
    "    loss_hist = histogram_similarity(fake_B, real_B)\n",
    "    return loss_GAN + lambda_pixel * loss_pixel + 0.4 * loss_ssim + 0.4 * loss_hist\n",
    "\n",
    "# 초기화 부분\n",
    "best_score = float('-inf')\n",
    "\n",
    "# 학습 및 검증 루프\n",
    "for epoch in range(1, CFG['EPOCHS'] + 1):\n",
    "    generator.train()  # 학습 모드\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        real_A = batch['A'].to(device)\n",
    "        real_B = batch['B'].to(device)\n",
    "\n",
    "        # Generator 훈련\n",
    "        optimizer_G.zero_grad()\n",
    "        fake_B = generator(real_A)\n",
    "        pred_fake = discriminator(fake_B, real_A)\n",
    "        loss_GAN = criterion_GAN(pred_fake, torch.ones_like(pred_fake).to(device))\n",
    "        loss_pixel = criterion_pixelwise(fake_B, real_B)\n",
    "        loss_G = loss_GAN + lambda_pixel * loss_pixel\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # Discriminator 훈련\n",
    "        optimizer_D.zero_grad()\n",
    "        pred_real = discriminator(real_B, real_A)\n",
    "        loss_real = criterion_GAN(pred_real, torch.ones_like(pred_real).to(device))\n",
    "        pred_fake = discriminator(fake_B.detach(), real_A)\n",
    "        loss_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake).to(device))\n",
    "        loss_D = 0.5 * (loss_real + loss_fake)\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Discriminator 정확도 계산\n",
    "        real_accuracy = torch.mean((pred_real > 0.5).float())\n",
    "        fake_accuracy = torch.mean((pred_fake < 0.5).float())\n",
    "        accuracy = 0.5 * (real_accuracy + fake_accuracy)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"[Epoch {epoch}/{CFG['EPOCHS']}] [Batch {i}/{len(train_dataloader)}] \"\n",
    "                  f\"[D loss: {loss_D.item()}] [G loss: {loss_G.item()}] [D accuracy: {accuracy.item() * 100:.2f}%]\")\n",
    "\n",
    "    # Validation 단계\n",
    "    generator.eval()  # 검증 모드\n",
    "    validation_loss = 0.0\n",
    "    ssim_scores, masked_ssim_scores, hist_similarities = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for j, val_batch in enumerate(validation_dataloader):\n",
    "            real_val_A = val_batch['A'].to(device)\n",
    "            real_val_B = val_batch['B'].to(device)\n",
    "            fake_val_B = generator(real_val_A)\n",
    "\n",
    "            # 각각의 스코어 계산\n",
    "            ssim_score = ssim(fake_val_B, real_val_B, data_range=1.0).item()\n",
    "            masked_ssim_score = ssim(fake_val_B * real_val_A, real_val_B * real_val_A, data_range=1.0).item()\n",
    "            hist_similarity = histogram_similarity(fake_val_B, real_val_B)\n",
    "\n",
    "            ssim_scores.append(ssim_score)\n",
    "            masked_ssim_scores.append(masked_ssim_score)\n",
    "            hist_similarities.append(hist_similarity)\n",
    "\n",
    "            val_loss = combined_loss(fake_val_B, real_val_B)\n",
    "            validation_loss += val_loss.item()\n",
    "    \n",
    "     # 평균 스코어 계산\n",
    "    S = sum(ssim_scores) / len(ssim_scores)\n",
    "    M = sum(masked_ssim_scores) / len(masked_ssim_scores)\n",
    "    C = sum(hist_similarities) / len(hist_similarities)\n",
    "    score = (0.2 * S) + (0.4 * M) + (0.4 * C)\n",
    "\n",
    "    validation_loss /= len(validation_dataloader)\n",
    "    validation_log_message = f\"Validation loss: {validation_loss}, Score: {score}\"\n",
    "    print(validation_log_message)\n",
    "\n",
    "    # 텍스트 파일에 Validation 손실 기록\n",
    "    with open(log_file, \"a\") as log:\n",
    "        log.write(validation_log_message + \"\\n\")\n",
    "\n",
    "    # SSIM 평가 기준에 따른 최적 모델 저장\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        torch.save(generator.state_dict(), os.path.join(model_save_dir, \"best_generator.pth\"))\n",
    "        torch.save(discriminator.state_dict(), os.path.join(model_save_dir, \"best_discriminator.pth\"))\n",
    "        save_message = f\"Best model saved at epoch {epoch} with Score: {score}\"\n",
    "        print(save_message)\n",
    "        with open(log_file, \"a\") as log:\n",
    "            log.write(save_message + \"\\n\")\n",
    "\n",
    "# 학습 완료 후 최고의 스코어 출력\n",
    "final_message = f\"Training completed. Best Score: {best_score}\"\n",
    "print(final_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T10:30:35.761775Z",
     "iopub.status.busy": "2024-11-14T10:30:35.760948Z",
     "iopub.status.idle": "2024-11-14T10:30:42.079284Z",
     "shell.execute_reply": "2024-11-14T10:30:42.077384Z",
     "shell.execute_reply.started": "2024-11-14T10:30:35.761735Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'UNetPP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 22\u001b[0m\n\u001b[0;32m     18\u001b[0m generator_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaved_models/best_model-epoch=01-val_score=0.0000.ckpt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# 모델 로드 및 설정\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# model = UNetPP(in_channels=3, out_channels=3).to(device)  # UNetPP로 설정\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[43mUNetPP\u001b[49m\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(generator_path))\n\u001b[0;32m     23\u001b[0m UNetPP\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# 파일 리스트 불러오기\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'UNetPP' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 저장할 디렉토리 설정\n",
    "submission_dir = \"./submission3\"\n",
    "os.makedirs(submission_dir, exist_ok=True)\n",
    "\n",
    "# 이미지 로드 및 전처리\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image)\n",
    "    image = image.unsqueeze(0)  # 배치 차원을 추가합니다.\n",
    "    return image\n",
    "\n",
    "# 모델 경로 설정\n",
    "generator_path = 'saved_models/best_model-epoch=01-val_score=0.0000.ckpt'\n",
    "\n",
    "# 모델 로드 및 설정\n",
    "# model = UNetPP(in_channels=3, out_channels=3).to(device)  # UNetPP로 설정\n",
    "UNetPP.load_state_dict(torch.load(generator_path))\n",
    "UNetPP.eval()\n",
    "\n",
    "# 파일 리스트 불러오기\n",
    "test_images = sorted(os.listdir(test_dir))\n",
    "\n",
    "# 모든 테스트 이미지에 대해 추론 수행\n",
    "for image_name in test_images:\n",
    "    test_image_path = os.path.join(test_dir, image_name)\n",
    "\n",
    "    # 손상된 테스트 이미지 로드 및 전처리\n",
    "    test_image = load_image(test_image_path).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 모델로 예측\n",
    "        pred_image = UNetPP(test_image)\n",
    "        pred_image = pred_image.cpu().squeeze(0)  # 배치 차원 제거\n",
    "        pred_image = pred_image * 0.5 + 0.5  # 역정규화\n",
    "        pred_image = pred_image.numpy().transpose(1, 2, 0)  # HWC로 변경\n",
    "        pred_image = (pred_image * 255).astype('uint8')  # 0-255 범위로 변환\n",
    "        \n",
    "        # 예측된 이미지를 실제 이미지와 같은 512x512로 리사이즈\n",
    "        pred_image_resized = cv2.resize(pred_image, (512, 512), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # 결과 이미지 저장\n",
    "    output_path = os.path.join(submission_dir, image_name)\n",
    "    cv2.imwrite(output_path, cv2.cvtColor(pred_image_resized, cv2.COLOR_RGB2BGR))    \n",
    "    \n",
    "print(f\"Saved all images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T10:30:44.383678Z",
     "iopub.status.busy": "2024-11-14T10:30:44.382705Z",
     "iopub.status.idle": "2024-11-14T10:30:44.502335Z",
     "shell.execute_reply": "2024-11-14T10:30:44.501356Z",
     "shell.execute_reply.started": "2024-11-14T10:30:44.383635Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 저장된 결과 이미지를 ZIP 파일로 압축\n",
    "zip_filename = \"submission2.zip\"\n",
    "with zipfile.ZipFile(zip_filename, 'w') as submission_zip:\n",
    "    for image_name in test_images:\n",
    "        image_path = os.path.join(submission_dir, image_name)\n",
    "        submission_zip.write(image_path, arcname=image_name)\n",
    "\n",
    "print(f\"All images saved in {zip_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6078662,
     "sourceId": 9896364,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "tensorflow-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
