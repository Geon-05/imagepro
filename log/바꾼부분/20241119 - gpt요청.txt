1. CollateFn 클래스 정의 필요
train_dataloader와 validation_dataloader에서 collate_fn으로 CollateFn(mode='train')을 사용하고 있는데, CollateFn 클래스가 코드 내에 정의되어 있지 않습니다. 이를 정의하거나 불필요하다면 제거해야 합니다.
2. 데이터셋 이미지 크기와 배치 설정 관련
데이터셋 전처리에서 transforms.Resize((256, 256))을 사용하고 있으나, 학습에 사용되는 batch_size=CFG['BATCH_SIZE'] 설정(현재 32)이 GPU 메모리에 맞는지 확인해볼 필요가 있습니다. 이미지 크기가 크고 배치 사이즈가 크면 GPU 메모리 부족 오류가 발생할 수 있습니다.
3. use_masks의 적용 방식 개선 필요
CustomDataset 클래스에서 use_masks에 대한 옵션을 처리하지만, __getitem__ 함수에서 get_input_image(origin_img)가 정의되지 않아 사용되지 않는 것 같습니다. 해당 함수를 정의하거나 이 부분을 제거해야 합니다.
4. 모델 저장 경로 변수 설정 위치 조정
model_save_dir을 정의하고 있지만, trainer 객체보다 뒤에 위치하고 있어 ModelCheckpoint 콜백에서 사용할 때 해당 경로가 존재하지 않을 수 있습니다. model_save_dir을 더 앞부분에 정의하여 모델 저장 경로를 확실히 설정하도록 수정하는 것이 좋습니다.
5. CUDA 성능 최적화 관련
torch.backends.cudnn.benchmark = True와 torch.backends.cudnn.deterministic = True를 동시에 사용하고 있습니다. 이 두 가지 설정은 상충되는 목적을 가집니다. 최적화를 위해 deterministic을 사용해야 하는 경우, benchmark는 False로 설정해야 합니다. 둘 중 필요한 설정을 선택하고 맞추는 것이 좋습니다.
6. EarlyStopping 콜백과 훈련 단계의 불일치
trainer의 EarlyStopping 콜백에서 patience=3으로 설정되어 있지만, val_score 모니터링 값은 코드 상에 정의되지 않았습니다. 정확히 어떤 값을 val_score로 사용할지 명확히 정의해야 합니다. 예를 들어 score 값을 trainer에서 활용할 수 있도록 조정해야 합니다.
7. 모델 훈련 코드의 순서 및 분리 개선
모델 훈련 코드가 Trainer 객체와 GPU 학습 루프에서 혼합되어 있습니다. 이는 코드 유지보수 및 이해에 어려움을 줄 수 있습니다. 두 가지 방법 중 하나를 선택하여 사용하는 것이 좋습니다.
PyTorch Lightning Trainer를 사용하여 훈련을 진행하거나
torch 기반으로 직접 학습 루프를 구성하고 다른 훈련 관련 설정을 통합하는 방식으로 하나를 선택하는 것이 바람직합니다.
8. 검증 손실 계산 방식
validation_loss는 반복문의 각 배치 손실 값을 더하고 있지만, 최종적으로 이를 배치 수로 나눠 평균을 계산하는 단계가 없습니다. 이로 인해 검증 손실이 과도하게 크게 나타날 수 있습니다.
python
코드 복사
validation_loss /= len(validation_dataloader)
9. 불필요한 코드 및 중복된 라이브러리 제거
import 구문에서 여러 번 등장하는 torch, torch.nn, torch.optim, DataLoader 등이 중복되어 있습니다. 중복된 import를 제거하여 가독성을 높이는 것이 좋습니다.
10. masked_ssim_score 함수의 데이터 크기 조정 오류
masked_ssim_score 함수에서 ssim을 계산할 때 true_masked와 pred_masked 배열의 크기가 동일하지 않을 가능성이 있습니다. 데이터가 마스크된 후의 크기를 확인하고 동일하게 맞춰야 합니다.
이러한 수정 사항들을 고려하여 코드를 개선하면 전체적인 학습 과정이 더욱 안정적으로 진행될 것이며, 효율성도 향상될 것입니다. 필요한 부분을 더 설명하거나 구체적으로 수정하는 데 도움을 줄 수 있으니, 언제든 문의해주세요.