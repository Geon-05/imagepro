{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지 비슷한것끼리 묶어서 데이터 줄이는 용도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"geolocal/StreetCLIP\")\n",
    "clip_model.to('cuda')\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"geolocal/StreetCLIP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_embeddings(paths):\n",
    "    image_paths = sorted(glob(f'{paths}/*.png'))\n",
    "    image_features = []\n",
    "    for i in tqdm(range(0,len(image_paths), BATCH_SIZE)):\n",
    "        image_paths_batch = image_paths[i:i+BATCH_SIZE]\n",
    "        images = [Image.open(image_path) for image_path in image_paths_batch]\n",
    "        pixel_values = clip_processor.image_processor(images=images, return_tensors='pt')['pixel_values'].to('cuda')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            image_features_row = clip_model.get_image_features(pixel_values).cpu().numpy()\n",
    "        image_features.append(image_features_row)\n",
    "        \n",
    "    embeddings = np.vstack(image_features)  # 모든 배치를 하나의 배열로 통합\n",
    "    embeddings = normalize(embeddings, norm=\"l2\") # L2 정규화\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "BATCH_SIZE = 128"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
