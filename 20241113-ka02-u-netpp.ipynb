{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T12:34:20.623637Z",
     "iopub.status.busy": "2024-11-13T12:34:20.623248Z",
     "iopub.status.idle": "2024-11-13T12:34:26.258037Z",
     "shell.execute_reply": "2024-11-13T12:34:26.256919Z",
     "shell.execute_reply.started": "2024-11-13T12:34:20.623590Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import cv2\n",
    "\n",
    "import zipfile\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'EPOCHS':10,\n",
    "    'LEARNING_RATE':3e-4,\n",
    "    # 'BATCH_SIZE':16,\n",
    "    'BATCH_SIZE':128,\n",
    "    'SEED':42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#저장된 이미지 쌍을 동시에 로드 \n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, damage_dir, origin_dir, transform=None):\n",
    "        self.damage_dir = damage_dir\n",
    "        self.origin_dir = origin_dir\n",
    "        self.transform = transform\n",
    "        self.damage_files = sorted(os.listdir(damage_dir))\n",
    "        self.origin_files = sorted(os.listdir(origin_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.damage_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        damage_img_name = self.damage_files[idx]\n",
    "        origin_img_name = self.origin_files[idx]\n",
    "\n",
    "        damage_img_path = os.path.join(self.damage_dir, damage_img_name)\n",
    "        origin_img_path = os.path.join(self.origin_dir, origin_img_name)\n",
    "\n",
    "        damage_img = Image.open(damage_img_path).convert(\"RGB\")\n",
    "        origin_img = Image.open(origin_img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            damage_img = self.transform(damage_img)\n",
    "            origin_img = self.transform(origin_img)\n",
    "\n",
    "        return {'A': damage_img, 'B': origin_img}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 경로 설정\n",
    "origin_dir = 'data/train_gt'  # 원본 이미지 폴더 경로\n",
    "damage_dir = 'data/train_input'  # 손상된 이미지 폴더 경로\n",
    "test_dir = 'data/test_input'     # test 이미지 폴더 경로\n",
    "\n",
    "# 데이터 전처리 설정\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "# 데이터셋 및 DataLoader 생성\n",
    "dataset = CustomDataset(damage_dir=damage_dir, origin_dir=origin_dir, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# PatchGAN 기반의 Discriminator\n",
    "class PatchGANDiscriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(PatchGANDiscriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, normalization=True):\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, kernel_size=4, stride=2, padding=1)]\n",
    "            if normalization:\n",
    "                layers.append(nn.BatchNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return nn.Sequential(*layers)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            discriminator_block(in_channels * 2, 64, normalization=False),\n",
    "            discriminator_block(64, 128),\n",
    "            discriminator_block(128, 256),\n",
    "            discriminator_block(256, 512),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, img_A, img_B):\n",
    "        img_input = torch.cat((img_A, img_B), 1)\n",
    "        return self.model(img_input)\n",
    "\n",
    "# 가중치 초기화 함수\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm2d') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install segmentation-models-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from segmentation_models_pytorch import UnetPlusPlus\n",
    "\n",
    "# EfficientNet-B4를 백본으로 사용\n",
    "UNetPP = UnetPlusPlus(\n",
    "    encoder_name=\"efficientnet-b4\",  # pretrained on ImageNet\n",
    "    encoder_weights=\"imagenet\",     \n",
    "    in_channels=3,\n",
    "    classes=1                       \n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# U-Net++ 모델 정의\n",
    "generator = UNetPP(in_channels=3, out_channels=3)\n",
    "\n",
    "# 모델 저장을 위한 디렉토리 생성\n",
    "model_save_dir = \"./saved_models\"\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "lambda_pixel = 100  # 픽셀 손실에 대한 가중치\n",
    "\n",
    "# PatchGAN Discriminator 초기화\n",
    "discriminator = PatchGANDiscriminator()\n",
    "discriminator = discriminator.to(device)\n",
    "\n",
    "# 가중치 초기화\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n",
    "\n",
    "# 손실 함수 및 옵티마이저 설정\n",
    "criterion_GAN = nn.MSELoss()\n",
    "criterion_pixelwise = nn.L1Loss()\n",
    "\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=CFG[\"LEARNING_RATE\"])\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=CFG[\"LEARNING_RATE\"]) \n",
    "\n",
    "# 학습\n",
    "for epoch in range(1, CFG['EPOCHS'] + 1):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        real_A = batch['A'].to(device)\n",
    "        real_B = batch['B'].to(device)\n",
    "\n",
    "        # Generator 훈련\n",
    "        optimizer_G.zero_grad()\n",
    "        fake_B = generator(real_A)\n",
    "        pred_fake = discriminator(fake_B, real_A)\n",
    "        loss_GAN = criterion_GAN(pred_fake, torch.ones_like(pred_fake).to(device))\n",
    "        loss_pixel = criterion_pixelwise(fake_B, real_B)\n",
    "        loss_G = loss_GAN + lambda_pixel * loss_pixel\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # Discriminator 훈련\n",
    "        optimizer_D.zero_grad()\n",
    "        pred_real = discriminator(real_B, real_A)\n",
    "        loss_real = criterion_GAN(pred_real, torch.ones_like(pred_real).to(device))\n",
    "        pred_fake = discriminator(fake_B.detach(), real_A)\n",
    "        loss_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake).to(device))\n",
    "        loss_D = 0.5 * (loss_real + loss_fake)\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # 진행 상황 출력\n",
    "        print(f\"[Epoch {epoch}/{CFG['EPOCHS']}] [Batch {i}/{len(dataloader)}] [D loss: {loss_D.item()}] [G loss: {loss_G.item()}]\")\n",
    "\n",
    "        # 현재 에포크에서의 손실이 best_loss보다 작으면 모델 저장\n",
    "        if loss_G.item() < best_loss:\n",
    "            best_loss = loss_G.item()\n",
    "            torch.save(generator.state_dict(), os.path.join(model_save_dir, \"best_generator.pth\"))\n",
    "            torch.save(discriminator.state_dict(), os.path.join(model_save_dir, \"best_discriminator.pth\"))\n",
    "            print(f\"Best model saved at epoch {epoch}, batch {i} with G loss: {loss_G.item()} and D loss: {loss_D.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# 저장할 디렉토리 설정\n",
    "submission_dir = \"./submission\"\n",
    "os.makedirs(submission_dir, exist_ok=True)\n",
    "\n",
    "# 이미지 로드 및 전처리\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image)\n",
    "    image = image.unsqueeze(0)  # 배치 차원을 추가합니다.\n",
    "    return image\n",
    "\n",
    "# 모델 경로 설정\n",
    "generator_path = os.path.join(model_save_dir, \"best_generator.pth\")\n",
    "\n",
    "# 모델 로드 및 설정\n",
    "model = UNetPP(in_channels=3, out_channels=3).to(device)  # UNetPP로 설정\n",
    "model.load_state_dict(torch.load(generator_path))\n",
    "model.eval()\n",
    "\n",
    "# 파일 리스트 불러오기\n",
    "test_images = sorted(os.listdir(test_dir))\n",
    "\n",
    "# 모든 테스트 이미지에 대해 추론 수행\n",
    "for image_name in test_images:\n",
    "    test_image_path = os.path.join(test_dir, image_name)\n",
    "\n",
    "    # 손상된 테스트 이미지 로드 및 전처리\n",
    "    test_image = load_image(test_image_path).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 모델로 예측\n",
    "        pred_image = model(test_image)\n",
    "        pred_image = pred_image.cpu().squeeze(0)  # 배치 차원 제거\n",
    "        pred_image = pred_image * 0.5 + 0.5  # 역정규화\n",
    "        pred_image = pred_image.numpy().transpose(1, 2, 0)  # HWC로 변경\n",
    "        pred_image = (pred_image * 255).astype('uint8')  # 0-255 범위로 변환\n",
    "        \n",
    "        # 예측된 이미지를 실제 이미지와 같은 512x512로 리사이즈\n",
    "        pred_image_resized = cv2.resize(pred_image, (512, 512), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # 결과 이미지 저장\n",
    "    output_path = os.path.join(submission_dir, image_name)\n",
    "    cv2.imwrite(output_path, cv2.cvtColor(pred_image_resized, cv2.COLOR_RGB2BGR))    \n",
    "    \n",
    "print(f\"Saved all images\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "imagepro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
