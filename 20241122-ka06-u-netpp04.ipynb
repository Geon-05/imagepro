{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install segmentation-models-pytorch\n",
    "# !pip install pytorch_msssim\n",
    "# !pip install lightning\n",
    "# !pip install lightning[extra]\n",
    "# !pip install tensorboard\n",
    "# !pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.5.1-cp39-cp39-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting pillow\n",
      "  Downloading pillow-11.0.0-cp39-cp39-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.20.1-cp39-cp39-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/zqrc05/anaconda3/envs/imagepro/lib/python3.9/site-packages (from torch) (4.12.2)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: jinja2 in /home/zqrc05/anaconda3/envs/imagepro/lib/python3.9/site-packages (from torch) (3.1.4)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
      "  Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
      "  Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.1.0 (from torch)\n",
      "  Downloading triton-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/zqrc05/anaconda3/envs/imagepro/lib/python3.9/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.5.1-cp39-cp39-manylinux1_x86_64.whl (906.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.5/906.5 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hUsing cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Downloading triton-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.0.0-cp39-cp39-manylinux_2_28_x86_64.whl (4.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.20.1-cp39-cp39-manylinux1_x86_64.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.5 MB)\n",
      "Downloading numpy-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, fsspec, filelock, triton, opencv-python, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision\n",
      "Successfully installed filelock-3.16.1 fsspec-2024.10.0 mpmath-1.3.0 networkx-3.2.1 numpy-2.0.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 opencv-python-4.10.0.84 pillow-11.0.0 sympy-1.13.1 torch-2.5.1 torchvision-0.20.1 triton-3.1.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch pillow torchvision opencv-python numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import cv2\n",
    "\n",
    "import zipfile\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "0\n",
      "NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "12.4\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())  # True인지 확인\n",
    "print(torch.cuda.device_count())  # 사용 가능한 GPU 개수 확인\n",
    "print(torch.cuda.current_device())  # 현재 사용 중인 GPU 인덱스 확인\n",
    "print(torch.cuda.get_device_name(0))  # 사용 중인 GPU 이름 확인\n",
    "print(torch.version.cuda)  # PyTorch가 사용하는 CUDA 버전 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29603\n",
      "29603\n"
     ]
    }
   ],
   "source": [
    "origin_dir = 'train_gt'\n",
    "damage_dir = 'train_input'\n",
    "\n",
    "print(len(os.listdir(origin_dir)))\n",
    "print(len(os.listdir(damage_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'EPOCHS':2,\n",
    "    'LEARNING_RATE':3e-4,\n",
    "    # 'BATCH_SIZE':16,\n",
    "    'BATCH_SIZE':16,\n",
    "    'SEED':42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True  # 결정론적 연산 보장\n",
    "    torch.backends.cudnn.benchmark = False     # 성능 최적화 대신 일관성 우선\n",
    "\n",
    "seed_everything(CFG['SEED'])  # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random: 0.6394267984578837\n",
      "NumPy Random: [0.37454012]\n",
      "PyTorch Random: tensor([0.8823])\n"
     ]
    }
   ],
   "source": [
    "seed_everything(42)\n",
    "\n",
    "# 테스트 난수\n",
    "print(\"Random:\", random.random())\n",
    "print(\"NumPy Random:\", np.random.rand(1))\n",
    "print(\"PyTorch Random:\", torch.rand(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_image(damage_img_path, origin_img_path):\n",
    "    # OpenCV로 이미지 읽기 (NumPy 배열로 읽음)\n",
    "    color_image = cv2.imread(origin_img_path)\n",
    "    gray_image = cv2.imread(damage_img_path, cv2.IMREAD_GRAYSCALE)  # 흑백 이미지로 읽기\n",
    "    \n",
    "    # 색상 이미지를 흑백으로 변환 (PIL로 변환 후 NumPy로 변환)\n",
    "    color_image_gray = cv2.cvtColor(color_image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # 두 이미지의 차이 계산\n",
    "    difference = cv2.absdiff(color_image_gray, gray_image)\n",
    "    \n",
    "    # 차이 값을 임계값으로 처리하여 이진화 이미지 생성\n",
    "    _, binary_difference = cv2.threshold(difference, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # 마스크 생성\n",
    "    mask = binary_difference > 0  # 차이가 있는 부분을 마스크로 설정\n",
    "    mask = Image.fromarray(mask.astype(np.uint8) * 255)  # 마스크 이미지를 PIL 형식으로 변환\n",
    "\n",
    "    return {\n",
    "        'image_gray_masked': Image.fromarray(gray_image),  # 손상된 이미지를 PIL 이미지로 반환\n",
    "        'mask': transforms.ToTensor()(mask)  # 마스크를 텐서로 변환하여 사용\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, damage_dir, origin_dir, transform=None, use_masks=True):\n",
    "        self.damage_dir = damage_dir\n",
    "        self.origin_dir = origin_dir\n",
    "        self.transform = transform\n",
    "        self.use_masks = use_masks\n",
    "        self.damage_files = sorted(os.listdir(damage_dir), key=lambda x: x.lower())\n",
    "        self.origin_files = sorted(os.listdir(origin_dir), key=lambda x: x.lower())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.damage_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        damage_img_name = self.damage_files[idx]\n",
    "        origin_img_name = self.origin_files[idx]\n",
    "\n",
    "        damage_img_path = os.path.join(self.damage_dir, damage_img_name)\n",
    "        origin_img_path = os.path.join(self.origin_dir, origin_img_name)\n",
    "\n",
    "        damage_img = Image.open(damage_img_path).convert(\"RGB\")\n",
    "        origin_img = Image.open(origin_img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.use_masks:\n",
    "            input_data = get_input_image(damage_img_path, origin_img_path)\n",
    "            mask = input_data['mask']\n",
    "            # `mask`가 이미 텐서인지 확인하고 변환 처리\n",
    "            if not isinstance(mask, torch.Tensor):\n",
    "                mask = transforms.ToTensor()(mask)\n",
    "        else:\n",
    "            mask = torch.zeros((1, damage_img.size[1], damage_img.size[0]))\n",
    "\n",
    "        if self.transform:\n",
    "            damage_img = self.transform(damage_img)\n",
    "            origin_img = self.transform(origin_img)\n",
    "\n",
    "        return {'A': damage_img, 'B': origin_img, 'mask': mask}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo:\n",
    "# 1. val_score\n",
    "# 2. get_input_image(image) 테스트해서 원본 (손상된 흑백)과 마스크(512*512로 나오되 배경은 다 0, 마스크부분은 회색)\n",
    "# 3. loss 잘못됨 (아마 컬러를 흑백으로 바꾸면 걔를 정답으로 써도 될듯)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_step의 loss계산부분 변경 -> 원본 컬러사진을 흑백으로 변환해서 비고하는 것으로\n",
    "# validation_step의 score 반환부분 변경 -> 학습완료시 최적의 모델 저장이 되지않는 부분으로 실제 학습에 적용이 되는지 확인필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation_models_pytorch import UnetPlusPlus\n",
    "import torch.nn.functional as F\n",
    "import lightning as L\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 히스토그램 유사도 계산 함수\n",
    "def get_histogram_similarity(true_np, pred_np, color_space=cv2.COLOR_RGB2HSV):\n",
    "    true_hsv = cv2.cvtColor(true_np.astype(np.uint8), color_space)\n",
    "    pred_hsv = cv2.cvtColor(pred_np.astype(np.uint8), color_space)\n",
    "    \n",
    "    hist_true = cv2.calcHist([true_hsv], [0], None, [180], [0, 180])\n",
    "    hist_pred = cv2.calcHist([pred_hsv], [0], None, [180], [0, 180])\n",
    "    \n",
    "    hist_true = cv2.normalize(hist_true, hist_true).flatten()\n",
    "    hist_pred = cv2.normalize(hist_pred, hist_pred).flatten()\n",
    "    \n",
    "    similarity = cv2.compareHist(hist_true, hist_pred, cv2.HISTCMP_CORREL)\n",
    "    return similarity\n",
    "\n",
    "# 마스크된 부분만 SSIM 계산\n",
    "def get_masked_ssim_score(true_np, pred_np, mask_np):\n",
    "    if mask_np.ndim == 3 and mask_np.shape[0] == 1:\n",
    "        mask_np = mask_np.squeeze(0)\n",
    "    elif mask_np.ndim == 3 and mask_np.shape[-1] == 1:\n",
    "        mask_np = mask_np.squeeze(-1)\n",
    "    \n",
    "    if true_np.shape[:2] != mask_np.shape:\n",
    "        mask_np = cv2.resize(mask_np, (true_np.shape[1], true_np.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    true_masked = true_np[mask_np > 0]\n",
    "    pred_masked = pred_np[mask_np > 0]\n",
    "\n",
    "    if true_masked.size == 0 or pred_masked.size == 0:\n",
    "        return 0\n",
    "\n",
    "    ssim_value = ssim(\n",
    "        true_masked, pred_masked, data_range=pred_masked.max() - pred_masked.min(), channel_axis=-1\n",
    "    )\n",
    "    return ssim_value\n",
    "\n",
    "# SSIM 계산 함수\n",
    "def get_ssim_score(true_np, pred_np):\n",
    "    ssim_value = ssim(\n",
    "        true_np, pred_np, channel_axis=-1, data_range=pred_np.max() - pred_np.min()\n",
    "    )\n",
    "    if np.isnan(ssim_value):\n",
    "        ssim_value = 0\n",
    "    return ssim_value\n",
    "\n",
    "# Lightning Module 정의\n",
    "class LitIRModel(L.LightningModule):\n",
    "    def __init__(self, model_1, model_2, image_mean=0.5, image_std=0.5):\n",
    "        super().__init__()\n",
    "        self.model_1 = model_1\n",
    "        self.model_2 = model_2\n",
    "        self.image_mean = image_mean\n",
    "        self.image_std = image_std\n",
    "\n",
    "    def forward(self, images_gray_masked):\n",
    "        images_gray_restored = self.model_1(images_gray_masked) + images_gray_masked\n",
    "        images_restored = self.model_2(images_gray_restored)\n",
    "        return images_gray_restored, images_restored\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=1e-4)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # 채널 확인\n",
    "        assert batch['A'].shape[1] == 3, \"batch['A']는 3채널 RGB 이미지여야 합니다.\"\n",
    "        assert batch['B'].shape[1] == 3, \"batch['B']는 3채널 Ground Truth 이미지여야 합니다.\"\n",
    "\n",
    "        # 손상된 이미지 (Gray Scale)\n",
    "        images_gray_masked = torch.mean(batch['A'], dim=1, keepdim=True)\n",
    "        images_gt = batch['B']\n",
    "        \n",
    "        # 모델 입력\n",
    "        images_gray_restored, images_restored = self(images_gray_masked)\n",
    "        \n",
    "        # 손실 계산\n",
    "        loss_pixel_gray = (\n",
    "            F.l1_loss(images_gray_masked, images_gray_restored, reduction='mean') * 0.5 +\n",
    "            F.mse_loss(images_gray_masked, images_gray_restored, reduction='mean') * 0.5\n",
    "        )\n",
    "        loss_pixel = (\n",
    "            F.l1_loss(images_gt, images_restored, reduction='mean') * 0.5 +\n",
    "            F.mse_loss(images_gt, images_restored, reduction='mean') * 0.5\n",
    "        )\n",
    "        loss = loss_pixel_gray * 0.5 + loss_pixel * 0.5\n",
    "        \n",
    "        # 10번째 배치마다 로그 출력\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch: {self.current_epoch}, Batch: {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # 로깅\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images_gray_masked = torch.mean(batch['A'], dim=1, keepdim=True)\n",
    "        images_gt = batch['B']\n",
    "\n",
    "        images_gray_restored, images_restored = self(images_gray_masked)\n",
    "\n",
    "        # Ground Truth와 복원된 이미지 크기 맞추기\n",
    "        if images_restored.shape != images_gt.shape:\n",
    "            images_restored = torch.nn.functional.interpolate(\n",
    "                images_restored, size=images_gt.shape[-2:], mode=\"bilinear\", align_corners=False\n",
    "            )\n",
    "\n",
    "        # NumPy 변환\n",
    "        images_restored_np = images_restored.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "        images_gt_np = images_gt.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "\n",
    "        # SSIM 및 기타 메트릭 계산\n",
    "        total_ssim_score = get_ssim_score(images_gt_np[0], images_restored_np[0])\n",
    "        self.log(\"val_ssim\", total_ssim_score, on_step=False, on_epoch=True)\n",
    "        return {\"val_ssim\": total_ssim_score}\n",
    "\n",
    "# 모델 초기화\n",
    "model_1 = UnetPlusPlus(\n",
    "    encoder_name=\"efficientnet-b4\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=1,  # Grayscale 입력\n",
    "    classes=1\n",
    ")\n",
    "\n",
    "model_2 = UnetPlusPlus(\n",
    "    encoder_name=\"efficientnet-b4\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=1,  # Gray->Color 변환을 위해 1채널 사용\n",
    "    classes=3  # RGB 출력\n",
    ")\n",
    "\n",
    "lit_ir_model = LitIRModel(model_1=model_1, model_2=model_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type         | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | model_1 | UnetPlusPlus | 20.8 M | train\n",
      "1 | model_2 | UnetPlusPlus | 20.8 M | train\n",
      "-------------------------------------------------\n",
      "41.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "41.6 M    Total params\n",
      "166.499   Total estimated model params size (MB)\n",
      "1274      Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef24eabd08643d492fad299344117fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                       | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Loss: 0.6740\n",
      "Epoch: 0, Batch: 10, Loss: 0.4713\n",
      "Epoch: 0, Batch: 20, Loss: 0.3466\n",
      "Epoch: 0, Batch: 30, Loss: 0.2593\n",
      "Epoch: 0, Batch: 40, Loss: 0.1928\n",
      "Epoch: 0, Batch: 50, Loss: 0.1671\n",
      "Epoch: 0, Batch: 60, Loss: 0.1280\n",
      "Epoch: 0, Batch: 70, Loss: 0.1228\n",
      "Epoch: 0, Batch: 80, Loss: 0.1083\n",
      "Epoch: 0, Batch: 90, Loss: 0.0896\n",
      "Epoch: 0, Batch: 100, Loss: 0.1046\n",
      "Epoch: 0, Batch: 110, Loss: 0.0879\n",
      "Epoch: 0, Batch: 120, Loss: 0.0892\n",
      "Epoch: 0, Batch: 130, Loss: 0.0780\n",
      "Epoch: 0, Batch: 140, Loss: 0.0777\n",
      "Epoch: 0, Batch: 150, Loss: 0.0736\n",
      "Epoch: 0, Batch: 160, Loss: 0.0767\n",
      "Epoch: 0, Batch: 170, Loss: 0.0613\n",
      "Epoch: 0, Batch: 180, Loss: 0.0643\n",
      "Epoch: 0, Batch: 190, Loss: 0.0621\n",
      "Epoch: 0, Batch: 200, Loss: 0.0771\n",
      "Epoch: 0, Batch: 210, Loss: 0.0684\n",
      "Epoch: 0, Batch: 220, Loss: 0.0600\n",
      "Epoch: 0, Batch: 230, Loss: 0.0612\n",
      "Epoch: 0, Batch: 240, Loss: 0.0708\n",
      "Epoch: 0, Batch: 250, Loss: 0.0591\n",
      "Epoch: 0, Batch: 260, Loss: 0.0682\n",
      "Epoch: 0, Batch: 270, Loss: 0.0565\n",
      "Epoch: 0, Batch: 280, Loss: 0.0606\n",
      "Epoch: 0, Batch: 290, Loss: 0.0607\n",
      "Epoch: 0, Batch: 300, Loss: 0.0676\n",
      "Epoch: 0, Batch: 310, Loss: 0.0598\n",
      "Epoch: 0, Batch: 320, Loss: 0.0510\n",
      "Epoch: 0, Batch: 330, Loss: 0.0573\n",
      "Epoch: 0, Batch: 340, Loss: 0.0475\n",
      "Epoch: 0, Batch: 350, Loss: 0.0542\n",
      "Epoch: 0, Batch: 360, Loss: 0.0666\n",
      "Epoch: 0, Batch: 370, Loss: 0.0693\n",
      "Epoch: 0, Batch: 380, Loss: 0.0621\n",
      "Epoch: 0, Batch: 390, Loss: 0.0482\n",
      "Epoch: 0, Batch: 400, Loss: 0.0601\n",
      "Epoch: 0, Batch: 410, Loss: 0.0726\n",
      "Epoch: 0, Batch: 420, Loss: 0.0477\n",
      "Epoch: 0, Batch: 430, Loss: 0.0574\n",
      "Epoch: 0, Batch: 440, Loss: 0.0616\n",
      "Epoch: 0, Batch: 450, Loss: 0.0495\n",
      "Epoch: 0, Batch: 460, Loss: 0.0563\n",
      "Epoch: 0, Batch: 470, Loss: 0.0501\n",
      "Epoch: 0, Batch: 480, Loss: 0.0531\n",
      "Epoch: 0, Batch: 490, Loss: 0.0641\n",
      "Epoch: 0, Batch: 500, Loss: 0.0501\n",
      "Epoch: 0, Batch: 510, Loss: 0.0499\n",
      "Epoch: 0, Batch: 520, Loss: 0.0682\n",
      "Epoch: 0, Batch: 530, Loss: 0.0472\n",
      "Epoch: 0, Batch: 540, Loss: 0.0477\n",
      "Epoch: 0, Batch: 550, Loss: 0.0528\n",
      "Epoch: 0, Batch: 560, Loss: 0.0442\n",
      "Epoch: 0, Batch: 570, Loss: 0.0564\n",
      "Epoch: 0, Batch: 580, Loss: 0.0417\n",
      "Epoch: 0, Batch: 590, Loss: 0.0439\n",
      "Epoch: 0, Batch: 600, Loss: 0.0525\n",
      "Epoch: 0, Batch: 610, Loss: 0.0440\n",
      "Epoch: 0, Batch: 620, Loss: 0.0449\n",
      "Epoch: 0, Batch: 630, Loss: 0.0448\n",
      "Epoch: 0, Batch: 640, Loss: 0.0442\n",
      "Epoch: 0, Batch: 650, Loss: 0.0497\n",
      "Epoch: 0, Batch: 660, Loss: 0.0436\n",
      "Epoch: 0, Batch: 670, Loss: 0.0455\n",
      "Epoch: 0, Batch: 680, Loss: 0.0492\n",
      "Epoch: 0, Batch: 690, Loss: 0.0453\n",
      "Epoch: 0, Batch: 700, Loss: 0.0384\n",
      "Epoch: 0, Batch: 710, Loss: 0.0429\n",
      "Epoch: 0, Batch: 720, Loss: 0.0402\n",
      "Epoch: 0, Batch: 730, Loss: 0.0484\n",
      "Epoch: 0, Batch: 740, Loss: 0.0423\n",
      "Epoch: 0, Batch: 750, Loss: 0.0515\n",
      "Epoch: 0, Batch: 760, Loss: 0.0534\n",
      "Epoch: 0, Batch: 770, Loss: 0.0414\n",
      "Epoch: 0, Batch: 780, Loss: 0.0435\n",
      "Epoch: 0, Batch: 790, Loss: 0.0409\n",
      "Epoch: 0, Batch: 800, Loss: 0.0396\n",
      "Epoch: 0, Batch: 810, Loss: 0.0389\n",
      "Epoch: 0, Batch: 820, Loss: 0.0352\n",
      "Epoch: 0, Batch: 830, Loss: 0.0377\n",
      "Epoch: 0, Batch: 840, Loss: 0.0355\n",
      "Epoch: 0, Batch: 850, Loss: 0.0364\n",
      "Epoch: 0, Batch: 860, Loss: 0.0396\n",
      "Epoch: 0, Batch: 870, Loss: 0.0362\n",
      "Epoch: 0, Batch: 880, Loss: 0.0454\n",
      "Epoch: 0, Batch: 890, Loss: 0.0494\n",
      "Epoch: 0, Batch: 900, Loss: 0.0455\n",
      "Epoch: 0, Batch: 910, Loss: 0.0427\n",
      "Epoch: 0, Batch: 920, Loss: 0.0431\n",
      "Epoch: 0, Batch: 930, Loss: 0.0363\n",
      "Epoch: 0, Batch: 940, Loss: 0.0491\n",
      "Epoch: 0, Batch: 950, Loss: 0.0412\n",
      "Epoch: 0, Batch: 960, Loss: 0.0379\n",
      "Epoch: 0, Batch: 970, Loss: 0.0369\n",
      "Epoch: 0, Batch: 980, Loss: 0.0380\n",
      "Epoch: 0, Batch: 990, Loss: 0.0393\n",
      "Epoch: 0, Batch: 1000, Loss: 0.0319\n",
      "Epoch: 0, Batch: 1010, Loss: 0.0359\n",
      "Epoch: 0, Batch: 1020, Loss: 0.0624\n",
      "Epoch: 0, Batch: 1030, Loss: 0.0520\n",
      "Epoch: 0, Batch: 1040, Loss: 0.0469\n",
      "Epoch: 0, Batch: 1050, Loss: 0.0375\n",
      "Epoch: 0, Batch: 1060, Loss: 0.0372\n",
      "Epoch: 0, Batch: 1070, Loss: 0.0451\n",
      "Epoch: 0, Batch: 1080, Loss: 0.0413\n",
      "Epoch: 0, Batch: 1090, Loss: 0.0441\n",
      "Epoch: 0, Batch: 1100, Loss: 0.0429\n",
      "Epoch: 0, Batch: 1110, Loss: 0.0344\n",
      "Epoch: 0, Batch: 1120, Loss: 0.0363\n",
      "Epoch: 0, Batch: 1130, Loss: 0.0357\n",
      "Epoch: 0, Batch: 1140, Loss: 0.0425\n",
      "Epoch: 0, Batch: 1150, Loss: 0.0454\n",
      "Epoch: 0, Batch: 1160, Loss: 0.0428\n",
      "Epoch: 0, Batch: 1170, Loss: 0.0387\n",
      "Epoch: 0, Batch: 1180, Loss: 0.0352\n",
      "Epoch: 0, Batch: 1190, Loss: 0.0402\n",
      "Epoch: 0, Batch: 1200, Loss: 0.0622\n",
      "Epoch: 0, Batch: 1210, Loss: 0.0351\n",
      "Epoch: 0, Batch: 1220, Loss: 0.0325\n",
      "Epoch: 0, Batch: 1230, Loss: 0.0444\n",
      "Epoch: 0, Batch: 1240, Loss: 0.0319\n",
      "Epoch: 0, Batch: 1250, Loss: 0.0339\n",
      "Epoch: 0, Batch: 1260, Loss: 0.0424\n",
      "Epoch: 0, Batch: 1270, Loss: 0.0349\n",
      "Epoch: 0, Batch: 1280, Loss: 0.0399\n",
      "Epoch: 0, Batch: 1290, Loss: 0.0476\n",
      "Epoch: 0, Batch: 1300, Loss: 0.0480\n",
      "Epoch: 0, Batch: 1310, Loss: 0.0361\n",
      "Epoch: 0, Batch: 1320, Loss: 0.0384\n",
      "Epoch: 0, Batch: 1330, Loss: 0.0360\n",
      "Epoch: 0, Batch: 1340, Loss: 0.0366\n",
      "Epoch: 0, Batch: 1350, Loss: 0.0390\n",
      "Epoch: 0, Batch: 1360, Loss: 0.0460\n",
      "Epoch: 0, Batch: 1370, Loss: 0.0512\n",
      "Epoch: 0, Batch: 1380, Loss: 0.0439\n",
      "Epoch: 0, Batch: 1390, Loss: 0.0416\n",
      "Epoch: 0, Batch: 1400, Loss: 0.0330\n",
      "Epoch: 0, Batch: 1410, Loss: 0.0411\n",
      "Epoch: 0, Batch: 1420, Loss: 0.0451\n",
      "Epoch: 0, Batch: 1430, Loss: 0.0401\n",
      "Epoch: 0, Batch: 1440, Loss: 0.0348\n",
      "Epoch: 0, Batch: 1450, Loss: 0.0376\n",
      "Epoch: 0, Batch: 1460, Loss: 0.0396\n",
      "Epoch: 0, Batch: 1470, Loss: 0.0384\n",
      "Epoch: 0, Batch: 1480, Loss: 0.0748\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 0, Loss: 0.0481\n",
      "Epoch: 1, Batch: 10, Loss: 0.0370\n",
      "Epoch: 1, Batch: 20, Loss: 0.0323\n",
      "Epoch: 1, Batch: 30, Loss: 0.0352\n",
      "Epoch: 1, Batch: 40, Loss: 0.0444\n",
      "Epoch: 1, Batch: 50, Loss: 0.0300\n",
      "Epoch: 1, Batch: 60, Loss: 0.0319\n",
      "Epoch: 1, Batch: 70, Loss: 0.0342\n",
      "Epoch: 1, Batch: 80, Loss: 0.0405\n",
      "Epoch: 1, Batch: 90, Loss: 0.0394\n",
      "Epoch: 1, Batch: 100, Loss: 0.0408\n",
      "Epoch: 1, Batch: 110, Loss: 0.0353\n",
      "Epoch: 1, Batch: 120, Loss: 0.0447\n",
      "Epoch: 1, Batch: 130, Loss: 0.0331\n",
      "Epoch: 1, Batch: 140, Loss: 0.0384\n",
      "Epoch: 1, Batch: 150, Loss: 0.0311\n",
      "Epoch: 1, Batch: 160, Loss: 0.0530\n",
      "Epoch: 1, Batch: 170, Loss: 0.0665\n",
      "Epoch: 1, Batch: 180, Loss: 0.0315\n",
      "Epoch: 1, Batch: 190, Loss: 0.0366\n",
      "Epoch: 1, Batch: 200, Loss: 0.0444\n",
      "Epoch: 1, Batch: 210, Loss: 0.0387\n",
      "Epoch: 1, Batch: 220, Loss: 0.0384\n",
      "Epoch: 1, Batch: 230, Loss: 0.0279\n",
      "Epoch: 1, Batch: 240, Loss: 0.0345\n",
      "Epoch: 1, Batch: 250, Loss: 0.0308\n",
      "Epoch: 1, Batch: 260, Loss: 0.0303\n",
      "Epoch: 1, Batch: 270, Loss: 0.0373\n",
      "Epoch: 1, Batch: 280, Loss: 0.0475\n",
      "Epoch: 1, Batch: 290, Loss: 0.0338\n",
      "Epoch: 1, Batch: 300, Loss: 0.0319\n",
      "Epoch: 1, Batch: 310, Loss: 0.0314\n",
      "Epoch: 1, Batch: 320, Loss: 0.0312\n",
      "Epoch: 1, Batch: 330, Loss: 0.0309\n",
      "Epoch: 1, Batch: 340, Loss: 0.0352\n",
      "Epoch: 1, Batch: 350, Loss: 0.0341\n",
      "Epoch: 1, Batch: 360, Loss: 0.0315\n",
      "Epoch: 1, Batch: 370, Loss: 0.0493\n",
      "Epoch: 1, Batch: 380, Loss: 0.0619\n",
      "Epoch: 1, Batch: 390, Loss: 0.0521\n",
      "Epoch: 1, Batch: 400, Loss: 0.0334\n",
      "Epoch: 1, Batch: 410, Loss: 0.0305\n",
      "Epoch: 1, Batch: 420, Loss: 0.0351\n",
      "Epoch: 1, Batch: 430, Loss: 0.0376\n",
      "Epoch: 1, Batch: 440, Loss: 0.0286\n",
      "Epoch: 1, Batch: 450, Loss: 0.0352\n",
      "Epoch: 1, Batch: 460, Loss: 0.0325\n",
      "Epoch: 1, Batch: 470, Loss: 0.0382\n",
      "Epoch: 1, Batch: 480, Loss: 0.0362\n",
      "Epoch: 1, Batch: 490, Loss: 0.0320\n",
      "Epoch: 1, Batch: 500, Loss: 0.0345\n",
      "Epoch: 1, Batch: 510, Loss: 0.0306\n",
      "Epoch: 1, Batch: 520, Loss: 0.0452\n",
      "Epoch: 1, Batch: 530, Loss: 0.0335\n",
      "Epoch: 1, Batch: 540, Loss: 0.0352\n",
      "Epoch: 1, Batch: 550, Loss: 0.0414\n",
      "Epoch: 1, Batch: 560, Loss: 0.0490\n",
      "Epoch: 1, Batch: 570, Loss: 0.0291\n",
      "Epoch: 1, Batch: 580, Loss: 0.0287\n",
      "Epoch: 1, Batch: 590, Loss: 0.0373\n",
      "Epoch: 1, Batch: 600, Loss: 0.0303\n",
      "Epoch: 1, Batch: 610, Loss: 0.0314\n",
      "Epoch: 1, Batch: 620, Loss: 0.0407\n",
      "Epoch: 1, Batch: 630, Loss: 0.0343\n",
      "Epoch: 1, Batch: 640, Loss: 0.0486\n",
      "Epoch: 1, Batch: 650, Loss: 0.0311\n",
      "Epoch: 1, Batch: 660, Loss: 0.0394\n",
      "Epoch: 1, Batch: 670, Loss: 0.0322\n",
      "Epoch: 1, Batch: 680, Loss: 0.0348\n",
      "Epoch: 1, Batch: 690, Loss: 0.0264\n",
      "Epoch: 1, Batch: 700, Loss: 0.0403\n",
      "Epoch: 1, Batch: 710, Loss: 0.0326\n",
      "Epoch: 1, Batch: 720, Loss: 0.0304\n",
      "Epoch: 1, Batch: 730, Loss: 0.0285\n",
      "Epoch: 1, Batch: 740, Loss: 0.0297\n",
      "Epoch: 1, Batch: 750, Loss: 0.0275\n",
      "Epoch: 1, Batch: 760, Loss: 0.0392\n",
      "Epoch: 1, Batch: 770, Loss: 0.0309\n",
      "Epoch: 1, Batch: 780, Loss: 0.0308\n",
      "Epoch: 1, Batch: 790, Loss: 0.0305\n",
      "Epoch: 1, Batch: 800, Loss: 0.0362\n",
      "Epoch: 1, Batch: 810, Loss: 0.0375\n",
      "Epoch: 1, Batch: 820, Loss: 0.0364\n",
      "Epoch: 1, Batch: 830, Loss: 0.0315\n",
      "Epoch: 1, Batch: 840, Loss: 0.0375\n",
      "Epoch: 1, Batch: 850, Loss: 0.0642\n",
      "Epoch: 1, Batch: 860, Loss: 0.0307\n",
      "Epoch: 1, Batch: 870, Loss: 0.0316\n",
      "Epoch: 1, Batch: 880, Loss: 0.0302\n",
      "Epoch: 1, Batch: 890, Loss: 0.0348\n",
      "Epoch: 1, Batch: 900, Loss: 0.0339\n",
      "Epoch: 1, Batch: 910, Loss: 0.0367\n",
      "Epoch: 1, Batch: 920, Loss: 0.0366\n",
      "Epoch: 1, Batch: 930, Loss: 0.0363\n",
      "Epoch: 1, Batch: 940, Loss: 0.0297\n",
      "Epoch: 1, Batch: 950, Loss: 0.0361\n",
      "Epoch: 1, Batch: 960, Loss: 0.0331\n",
      "Epoch: 1, Batch: 970, Loss: 0.0297\n",
      "Epoch: 1, Batch: 980, Loss: 0.0274\n",
      "Epoch: 1, Batch: 990, Loss: 0.0361\n",
      "Epoch: 1, Batch: 1000, Loss: 0.0290\n",
      "Epoch: 1, Batch: 1010, Loss: 0.0301\n",
      "Epoch: 1, Batch: 1020, Loss: 0.0314\n",
      "Epoch: 1, Batch: 1030, Loss: 0.0284\n",
      "Epoch: 1, Batch: 1040, Loss: 0.0327\n",
      "Epoch: 1, Batch: 1050, Loss: 0.0307\n",
      "Epoch: 1, Batch: 1060, Loss: 0.0336\n",
      "Epoch: 1, Batch: 1070, Loss: 0.0286\n",
      "Epoch: 1, Batch: 1080, Loss: 0.0395\n",
      "Epoch: 1, Batch: 1090, Loss: 0.0445\n",
      "Epoch: 1, Batch: 1100, Loss: 0.0330\n",
      "Epoch: 1, Batch: 1110, Loss: 0.0293\n",
      "Epoch: 1, Batch: 1120, Loss: 0.0349\n",
      "Epoch: 1, Batch: 1130, Loss: 0.0316\n",
      "Epoch: 1, Batch: 1140, Loss: 0.0316\n",
      "Epoch: 1, Batch: 1150, Loss: 0.0298\n",
      "Epoch: 1, Batch: 1160, Loss: 0.0267\n",
      "Epoch: 1, Batch: 1170, Loss: 0.0469\n",
      "Epoch: 1, Batch: 1180, Loss: 0.0361\n",
      "Epoch: 1, Batch: 1190, Loss: 0.0318\n",
      "Epoch: 1, Batch: 1200, Loss: 0.0288\n",
      "Epoch: 1, Batch: 1210, Loss: 0.0284\n",
      "Epoch: 1, Batch: 1220, Loss: 0.0319\n",
      "Epoch: 1, Batch: 1230, Loss: 0.0351\n",
      "Epoch: 1, Batch: 1240, Loss: 0.0366\n",
      "Epoch: 1, Batch: 1250, Loss: 0.0416\n",
      "Epoch: 1, Batch: 1260, Loss: 0.0349\n",
      "Epoch: 1, Batch: 1270, Loss: 0.0268\n",
      "Epoch: 1, Batch: 1280, Loss: 0.0338\n",
      "Epoch: 1, Batch: 1290, Loss: 0.0311\n",
      "Epoch: 1, Batch: 1300, Loss: 0.0354\n",
      "Epoch: 1, Batch: 1310, Loss: 0.0392\n",
      "Epoch: 1, Batch: 1320, Loss: 0.0349\n",
      "Epoch: 1, Batch: 1330, Loss: 0.0405\n",
      "Epoch: 1, Batch: 1340, Loss: 0.0323\n",
      "Epoch: 1, Batch: 1350, Loss: 0.0334\n",
      "Epoch: 1, Batch: 1360, Loss: 0.0350\n",
      "Epoch: 1, Batch: 1370, Loss: 0.0285\n",
      "Epoch: 1, Batch: 1380, Loss: 0.0454\n",
      "Epoch: 1, Batch: 1390, Loss: 0.0306\n",
      "Epoch: 1, Batch: 1400, Loss: 0.0472\n",
      "Epoch: 1, Batch: 1410, Loss: 0.0280\n",
      "Epoch: 1, Batch: 1420, Loss: 0.0444\n",
      "Epoch: 1, Batch: 1430, Loss: 0.0332\n",
      "Epoch: 1, Batch: 1440, Loss: 0.0272\n",
      "Epoch: 1, Batch: 1450, Loss: 0.0390\n",
      "Epoch: 1, Batch: 1460, Loss: 0.0308\n",
      "Epoch: 1, Batch: 1470, Loss: 0.0298\n",
      "Epoch: 1, Batch: 1480, Loss: 0.0608\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import lightning as L\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# 경로 설정\n",
    "origin_dir = 'train_gt'\n",
    "damage_dir = 'train_input'\n",
    "test_dir = 'test_input'\n",
    "\n",
    "# 데이터 전처리 설정\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # 모든 이미지를 256x256으로 리사이즈\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "# 데이터셋 생성\n",
    "dataset = CustomDataset(damage_dir=damage_dir, origin_dir=origin_dir, transform=transform)\n",
    "\n",
    "# 데이터셋 분할\n",
    "validation_ratio = 0.2\n",
    "train_size = int((1 - validation_ratio) * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "training_dataset, validation_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "class CollateFn:\n",
    "    def __init__(self, mode='train'):\n",
    "        self.mode = mode\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        A = torch.stack([item['A'] for item in batch])\n",
    "        B = torch.stack([item['B'] for item in batch])\n",
    "        masks = torch.stack([item['mask'] for item in batch]) if 'mask' in batch[0] else torch.zeros_like(A)\n",
    "\n",
    "        if self.mode in ['train', 'valid']:\n",
    "            return {'A': A, 'B': B, 'masks': masks}\n",
    "        elif self.mode == 'test':\n",
    "            return {'A': A}\n",
    "\n",
    "# CollateFn 정의\n",
    "train_collate_fn = CollateFn(mode='train')\n",
    "validation_collate_fn = CollateFn(mode='valid')\n",
    "\n",
    "# DataLoader 설정\n",
    "train_dataloader = DataLoader(\n",
    "    training_dataset,\n",
    "    batch_size=CFG['BATCH_SIZE'],\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    collate_fn=train_collate_fn\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size=CFG['BATCH_SIZE'],\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    collate_fn=validation_collate_fn\n",
    ")\n",
    "\n",
    "# 모델 저장 디렉토리 생성\n",
    "model_save_dir = \"./saved_models\"\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "# Trainer 설정 및 학습 시작\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=CFG['EPOCHS'],\n",
    "    precision=\"16-mixed\",\n",
    "    accelerator='gpu',\n",
    "    devices=1,\n",
    "    log_every_n_steps=10,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(\n",
    "            monitor='val_ssim',\n",
    "            mode='max',\n",
    "            save_top_k=1,\n",
    "            dirpath=model_save_dir,\n",
    "            filename='best_model-{epoch:02d}-{val_ssim:.4f}'\n",
    "        ),\n",
    "        EarlyStopping(monitor='val_ssim', mode='max', patience=3)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 학습 시작\n",
    "trainer.fit(lit_ir_model, train_dataloader, validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: output/output_0_0.png\n",
      "Saved: output/output_0_1.png\n",
      "Saved: output/output_0_2.png\n",
      "Saved: output/output_0_3.png\n",
      "Saved: output/output_0_4.png\n",
      "Saved: output/output_0_5.png\n",
      "Saved: output/output_0_6.png\n",
      "Saved: output/output_0_7.png\n",
      "Saved: output/output_0_8.png\n",
      "Saved: output/output_0_9.png\n",
      "Saved: output/output_0_10.png\n",
      "Saved: output/output_0_11.png\n",
      "Saved: output/output_0_12.png\n",
      "Saved: output/output_0_13.png\n",
      "Saved: output/output_0_14.png\n",
      "Saved: output/output_0_15.png\n",
      "Saved: output/output_1_0.png\n",
      "Saved: output/output_1_1.png\n",
      "Saved: output/output_1_2.png\n",
      "Saved: output/output_1_3.png\n",
      "Saved: output/output_1_4.png\n",
      "Saved: output/output_1_5.png\n",
      "Saved: output/output_1_6.png\n",
      "Saved: output/output_1_7.png\n",
      "Saved: output/output_1_8.png\n",
      "Saved: output/output_1_9.png\n",
      "Saved: output/output_1_10.png\n",
      "Saved: output/output_1_11.png\n",
      "Saved: output/output_1_12.png\n",
      "Saved: output/output_1_13.png\n",
      "Saved: output/output_1_14.png\n",
      "Saved: output/output_1_15.png\n",
      "Saved: output/output_2_0.png\n",
      "Saved: output/output_2_1.png\n",
      "Saved: output/output_2_2.png\n",
      "Saved: output/output_2_3.png\n",
      "Saved: output/output_2_4.png\n",
      "Saved: output/output_2_5.png\n",
      "Saved: output/output_2_6.png\n",
      "Saved: output/output_2_7.png\n",
      "Saved: output/output_2_8.png\n",
      "Saved: output/output_2_9.png\n",
      "Saved: output/output_2_10.png\n",
      "Saved: output/output_2_11.png\n",
      "Saved: output/output_2_12.png\n",
      "Saved: output/output_2_13.png\n",
      "Saved: output/output_2_14.png\n",
      "Saved: output/output_2_15.png\n",
      "Saved: output/output_3_0.png\n",
      "Saved: output/output_3_1.png\n",
      "Saved: output/output_3_2.png\n",
      "Saved: output/output_3_3.png\n",
      "Saved: output/output_3_4.png\n",
      "Saved: output/output_3_5.png\n",
      "Saved: output/output_3_6.png\n",
      "Saved: output/output_3_7.png\n",
      "Saved: output/output_3_8.png\n",
      "Saved: output/output_3_9.png\n",
      "Saved: output/output_3_10.png\n",
      "Saved: output/output_3_11.png\n",
      "Saved: output/output_3_12.png\n",
      "Saved: output/output_3_13.png\n",
      "Saved: output/output_3_14.png\n",
      "Saved: output/output_3_15.png\n",
      "Saved: output/output_4_0.png\n",
      "Saved: output/output_4_1.png\n",
      "Saved: output/output_4_2.png\n",
      "Saved: output/output_4_3.png\n",
      "Saved: output/output_4_4.png\n",
      "Saved: output/output_4_5.png\n",
      "Saved: output/output_4_6.png\n",
      "Saved: output/output_4_7.png\n",
      "Saved: output/output_4_8.png\n",
      "Saved: output/output_4_9.png\n",
      "Saved: output/output_4_10.png\n",
      "Saved: output/output_4_11.png\n",
      "Saved: output/output_4_12.png\n",
      "Saved: output/output_4_13.png\n",
      "Saved: output/output_4_14.png\n",
      "Saved: output/output_4_15.png\n",
      "Saved: output/output_5_0.png\n",
      "Saved: output/output_5_1.png\n",
      "Saved: output/output_5_2.png\n",
      "Saved: output/output_5_3.png\n",
      "Saved: output/output_5_4.png\n",
      "Saved: output/output_5_5.png\n",
      "Saved: output/output_5_6.png\n",
      "Saved: output/output_5_7.png\n",
      "Saved: output/output_5_8.png\n",
      "Saved: output/output_5_9.png\n",
      "Saved: output/output_5_10.png\n",
      "Saved: output/output_5_11.png\n",
      "Saved: output/output_5_12.png\n",
      "Saved: output/output_5_13.png\n",
      "Saved: output/output_5_14.png\n",
      "Saved: output/output_5_15.png\n",
      "Saved: output/output_6_0.png\n",
      "Saved: output/output_6_1.png\n",
      "Saved: output/output_6_2.png\n",
      "Saved: output/output_6_3.png\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 데이터 전처리\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "\n",
    "# 테스트 데이터셋 생성\n",
    "test_dataset = CustomDataset(damage_dir=test_dir, origin_dir=test_dir, transform=transform, use_masks=False)\n",
    "\n",
    "# 테스트 DataLoader 생성\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=CFG['BATCH_SIZE'],  # 적절히 설정\n",
    "    shuffle=False,\n",
    "    num_workers=2  # 경고를 피하기 위해 적절히 설정\n",
    ")\n",
    "\n",
    "# 모델 초기화\n",
    "model = LitIRModel.load_from_checkpoint(\n",
    "    checkpoint_path='saved_models/best_model-epoch=01-val_score=0.0000.ckpt',\n",
    "    model_1=model_1,\n",
    "    model_2=model_2\n",
    ")\n",
    "\n",
    "# 모델을 평가 모드로 설정\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# 테스트 데이터로 예측 실행\n",
    "output_dir = \"output/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "model.eval()  # 모델을 평가 모드로 설정\n",
    "with torch.no_grad():\n",
    "    for idx, batch in enumerate(test_dataloader):\n",
    "        # 입력 데이터 준비 (RGB -> Grayscale 변환)\n",
    "        inputs = torch.mean(batch['A'], dim=1, keepdim=True).to(device)  # [N, 1, H, W]\n",
    "        \n",
    "        # 모델 예측\n",
    "        gray_restored, color_restored = model(inputs)  # 모델 예측\n",
    "\n",
    "        # 예측된 이미지를 저장\n",
    "        for i, result in enumerate(color_restored):\n",
    "            result_img = (result.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)  # [C, H, W] -> [H, W, C]\n",
    "            output_path = os.path.join(output_dir, f\"output_{idx}_{i}.png\")\n",
    "            plt.imsave(output_path, result_img)\n",
    "            print(f\"Saved: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6075062,
     "sourceId": 9891694,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6078662,
     "sourceId": 9896364,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6111024,
     "sourceId": 9939640,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
