{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import random\nimport numpy as np\nimport os\n\nimport torch\nfrom PIL import Image\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch.optim as optim\nimport cv2\n\nimport zipfile\n\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T12:34:20.623248Z","iopub.execute_input":"2024-11-13T12:34:20.623637Z","iopub.status.idle":"2024-11-13T12:34:26.258037Z","shell.execute_reply.started":"2024-11-13T12:34:20.623590Z","shell.execute_reply":"2024-11-13T12:34:26.256919Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda:0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"CFG = {\n    'EPOCHS':10,\n    'LEARNING_RATE':3e-4,\n    # 'BATCH_SIZE':16,\n    'BATCH_SIZE':128,\n    'SEED':42\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(CFG['SEED']) # Seed 고정","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#저장된 이미지 쌍을 동시에 로드 \n\nclass CustomDataset(Dataset):\n    def __init__(self, damage_dir, origin_dir, transform=None):\n        self.damage_dir = damage_dir\n        self.origin_dir = origin_dir\n        self.transform = transform\n        self.damage_files = sorted(os.listdir(damage_dir))\n        self.origin_files = sorted(os.listdir(origin_dir))\n\n    def __len__(self):\n        return len(self.damage_files)\n\n    def __getitem__(self, idx):\n        damage_img_name = self.damage_files[idx]\n        origin_img_name = self.origin_files[idx]\n\n        damage_img_path = os.path.join(self.damage_dir, damage_img_name)\n        origin_img_path = os.path.join(self.origin_dir, origin_img_name)\n\n        damage_img = Image.open(damage_img_path).convert(\"RGB\")\n        origin_img = Image.open(origin_img_path).convert(\"RGB\")\n\n        if self.transform:\n            damage_img = self.transform(damage_img)\n            origin_img = self.transform(origin_img)\n\n        return {'A': damage_img, 'B': origin_img}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 경로 설정\norigin_dir = 'data/train_gt'  # 원본 이미지 폴더 경로\ndamage_dir = 'data/train_input'  # 손상된 이미지 폴더 경로\ntest_dir = 'data/test_input'     # test 이미지 폴더 경로\n\n# 데이터 전처리 설정\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5])\n])\n\n# 데이터셋 및 DataLoader 생성\ndataset = CustomDataset(damage_dir=damage_dir, origin_dir=origin_dir, transform=transform)\ndataloader = DataLoader(dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True, num_workers=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# PatchGAN 기반의 Discriminator\nclass PatchGANDiscriminator(nn.Module):\n    def __init__(self, in_channels=3):\n        super(PatchGANDiscriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, normalization=True):\n            layers = [nn.Conv2d(in_filters, out_filters, kernel_size=4, stride=2, padding=1)]\n            if normalization:\n                layers.append(nn.BatchNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return nn.Sequential(*layers)\n\n        self.model = nn.Sequential(\n            discriminator_block(in_channels * 2, 64, normalization=False),\n            discriminator_block(64, 128),\n            discriminator_block(128, 256),\n            discriminator_block(256, 512),\n            nn.Conv2d(512, 1, kernel_size=4, padding=1)\n        )\n\n    def forward(self, img_A, img_B):\n        img_input = torch.cat((img_A, img_B), 1)\n        return self.model(img_input)\n\n# 가중치 초기화 함수\ndef weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm2d') != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install segmentation-models-pytorch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from segmentation_models_pytorch import UnetPlusPlus\n\n# EfficientNet-B4를 백본으로 사용\nUNetPP = UnetPlusPlus(\n    encoder_name=\"efficientnet-b4\",  # pretrained on ImageNet\n    encoder_weights=\"imagenet\",     \n    in_channels=3,\n    classes=1                       \n).to(device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\n\n# U-Net++ 모델 정의\ngenerator = UNetPP(in_channels=3, out_channels=3)\n\n# 모델 저장을 위한 디렉토리 생성\nmodel_save_dir = \"./saved_models\"\nos.makedirs(model_save_dir, exist_ok=True)\n\nbest_loss = float(\"inf\")\nlambda_pixel = 100  # 픽셀 손실에 대한 가중치\n\n# PatchGAN Discriminator 초기화\ndiscriminator = PatchGANDiscriminator()\ndiscriminator = discriminator.to(device)\n\n# 가중치 초기화\ngenerator.apply(weights_init_normal)\ndiscriminator.apply(weights_init_normal)\n\n# 손실 함수 및 옵티마이저 설정\ncriterion_GAN = nn.MSELoss()\ncriterion_pixelwise = nn.L1Loss()\n\noptimizer_G = optim.Adam(generator.parameters(), lr=CFG[\"LEARNING_RATE\"])\noptimizer_D = optim.Adam(discriminator.parameters(), lr=CFG[\"LEARNING_RATE\"]) \n\n# 학습\nfor epoch in range(1, CFG['EPOCHS'] + 1):\n    for i, batch in enumerate(dataloader):\n        real_A = batch['A'].to(device)\n        real_B = batch['B'].to(device)\n\n        # Generator 훈련\n        optimizer_G.zero_grad()\n        fake_B = generator(real_A)\n        pred_fake = discriminator(fake_B, real_A)\n        loss_GAN = criterion_GAN(pred_fake, torch.ones_like(pred_fake).to(device))\n        loss_pixel = criterion_pixelwise(fake_B, real_B)\n        loss_G = loss_GAN + lambda_pixel * loss_pixel\n        loss_G.backward()\n        optimizer_G.step()\n\n        # Discriminator 훈련\n        optimizer_D.zero_grad()\n        pred_real = discriminator(real_B, real_A)\n        loss_real = criterion_GAN(pred_real, torch.ones_like(pred_real).to(device))\n        pred_fake = discriminator(fake_B.detach(), real_A)\n        loss_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake).to(device))\n        loss_D = 0.5 * (loss_real + loss_fake)\n        loss_D.backward()\n        optimizer_D.step()\n\n        # 진행 상황 출력\n        print(f\"[Epoch {epoch}/{CFG['EPOCHS']}] [Batch {i}/{len(dataloader)}] [D loss: {loss_D.item()}] [G loss: {loss_G.item()}]\")\n\n        # 현재 에포크에서의 손실이 best_loss보다 작으면 모델 저장\n        if loss_G.item() < best_loss:\n            best_loss = loss_G.item()\n            torch.save(generator.state_dict(), os.path.join(model_save_dir, \"best_generator.pth\"))\n            torch.save(discriminator.state_dict(), os.path.join(model_save_dir, \"best_discriminator.pth\"))\n            print(f\"Best model saved at epoch {epoch}, batch {i} with G loss: {loss_G.item()} and D loss: {loss_D.item()}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nfrom PIL import Image\nimport numpy as np\n\n# 저장할 디렉토리 설정\nsubmission_dir = \"./submission\"\nos.makedirs(submission_dir, exist_ok=True)\n\n# 이미지 로드 및 전처리\ndef load_image(image_path):\n    image = Image.open(image_path).convert(\"RGB\")\n    image = transform(image)\n    image = image.unsqueeze(0)  # 배치 차원을 추가합니다.\n    return image\n\n# 모델 경로 설정\ngenerator_path = os.path.join(model_save_dir, \"best_generator.pth\")\n\n# 모델 로드 및 설정\nmodel = UNetPP(in_channels=3, out_channels=3).to(device)  # UNetPP로 설정\nmodel.load_state_dict(torch.load(generator_path))\nmodel.eval()\n\n# 파일 리스트 불러오기\ntest_images = sorted(os.listdir(test_dir))\n\n# 모든 테스트 이미지에 대해 추론 수행\nfor image_name in test_images:\n    test_image_path = os.path.join(test_dir, image_name)\n\n    # 손상된 테스트 이미지 로드 및 전처리\n    test_image = load_image(test_image_path).to(device)\n\n    with torch.no_grad():\n        # 모델로 예측\n        pred_image = model(test_image)\n        pred_image = pred_image.cpu().squeeze(0)  # 배치 차원 제거\n        pred_image = pred_image * 0.5 + 0.5  # 역정규화\n        pred_image = pred_image.numpy().transpose(1, 2, 0)  # HWC로 변경\n        pred_image = (pred_image * 255).astype('uint8')  # 0-255 범위로 변환\n        \n        # 예측된 이미지를 실제 이미지와 같은 512x512로 리사이즈\n        pred_image_resized = cv2.resize(pred_image, (512, 512), interpolation=cv2.INTER_LINEAR)\n\n    # 결과 이미지 저장\n    output_path = os.path.join(submission_dir, image_name)\n    cv2.imwrite(output_path, cv2.cvtColor(pred_image_resized, cv2.COLOR_RGB2BGR))    \n    \nprint(f\"Saved all images\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}