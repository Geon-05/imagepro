{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting segmentation-models-pytorch\n",
      "  Using cached segmentation_models_pytorch-0.3.4-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting efficientnet-pytorch==0.7.1 (from segmentation-models-pytorch)\n",
      "  Using cached efficientnet_pytorch-0.7.1-py3-none-any.whl\n",
      "Collecting huggingface-hub>=0.24.6 (from segmentation-models-pytorch)\n",
      "  Using cached huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: pillow in c:\\users\\zqrc0\\anaconda3\\envs\\new_env\\lib\\site-packages (from segmentation-models-pytorch) (10.2.0)\n",
      "Collecting pretrainedmodels==0.7.4 (from segmentation-models-pytorch)\n",
      "  Using cached pretrainedmodels-0.7.4-py3-none-any.whl\n",
      "Requirement already satisfied: six in c:\\users\\zqrc0\\anaconda3\\envs\\new_env\\lib\\site-packages (from segmentation-models-pytorch) (1.16.0)\n",
      "Collecting timm==0.9.7 (from segmentation-models-pytorch)\n",
      "  Using cached timm-0.9.7-py3-none-any.whl.metadata (58 kB)\n",
      "Requirement already satisfied: torchvision>=0.5.0 in c:\\users\\zqrc0\\anaconda3\\envs\\new_env\\lib\\site-packages (from segmentation-models-pytorch) (0.20.1+cu118)\n",
      "Collecting tqdm (from segmentation-models-pytorch)\n",
      "  Using cached tqdm-4.67.0-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\zqrc0\\anaconda3\\envs\\new_env\\lib\\site-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.5.1+cu118)\n",
      "Collecting munch (from pretrainedmodels==0.7.4->segmentation-models-pytorch)\n",
      "  Using cached munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting pyyaml (from timm==0.9.7->segmentation-models-pytorch)\n",
      "  Using cached PyYAML-6.0.2-cp39-cp39-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting safetensors (from timm==0.9.7->segmentation-models-pytorch)\n",
      "  Using cached safetensors-0.4.5-cp39-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\zqrc0\\anaconda3\\envs\\new_env\\lib\\site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\zqrc0\\anaconda3\\envs\\new_env\\lib\\site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\zqrc0\\anaconda3\\envs\\new_env\\lib\\site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (24.2)\n",
      "Collecting requests (from huggingface-hub>=0.24.6->segmentation-models-pytorch)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\zqrc0\\anaconda3\\envs\\new_env\\lib\\site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (4.12.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\zqrc0\\anaconda3\\envs\\new_env\\lib\\site-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.26.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\zqrc0\\anaconda3\\envs\\new_env\\lib\\site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\zqrc0\\anaconda3\\envs\\new_env\\lib\\site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\zqrc0\\anaconda3\\envs\\new_env\\lib\\site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\zqrc0\\anaconda3\\envs\\new_env\\lib\\site-packages (from sympy==1.13.1->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\zqrc0\\anaconda3\\envs\\new_env\\lib\\site-packages (from tqdm->segmentation-models-pytorch) (0.4.6)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch)\n",
      "  Using cached charset_normalizer-3.4.0-cp39-cp39-win_amd64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch)\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch)\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\zqrc0\\anaconda3\\envs\\new_env\\lib\\site-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.1.5)\n",
      "Using cached segmentation_models_pytorch-0.3.4-py3-none-any.whl (109 kB)\n",
      "Using cached timm-0.9.7-py3-none-any.whl (2.2 MB)\n",
      "Using cached huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "Using cached tqdm-4.67.0-py3-none-any.whl (78 kB)\n",
      "Using cached PyYAML-6.0.2-cp39-cp39-win_amd64.whl (162 kB)\n",
      "Using cached munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached safetensors-0.4.5-cp39-none-win_amd64.whl (286 kB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Using cached charset_normalizer-3.4.0-cp39-cp39-win_amd64.whl (102 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Installing collected packages: urllib3, tqdm, safetensors, pyyaml, munch, idna, charset-normalizer, certifi, requests, huggingface-hub, efficientnet-pytorch, timm, pretrainedmodels, segmentation-models-pytorch\n",
      "Successfully installed certifi-2024.8.30 charset-normalizer-3.4.0 efficientnet-pytorch-0.7.1 huggingface-hub-0.26.2 idna-3.10 munch-4.0.0 pretrainedmodels-0.7.4 pyyaml-6.0.2 requests-2.32.3 safetensors-0.4.5 segmentation-models-pytorch-0.3.4 timm-0.9.7 tqdm-4.67.0 urllib3-2.2.3\n"
     ]
    }
   ],
   "source": [
    "# !pip install segmentation-models-pytorch\n",
    "# !pip install pytorch_msssim\n",
    "# !pip install scikit-image\n",
    "# !pip install torch\n",
    "# !pip install -r requirements.txt\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import cv2\n",
    "\n",
    "import zipfile\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'EPOCHS':2,\n",
    "    'LEARNING_RATE':3e-4,\n",
    "    # 'BATCH_SIZE':16,\n",
    "    'BATCH_SIZE':16,\n",
    "    'SEED':42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#저장된 이미지 쌍을 동시에 로드 \n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, damage_dir, origin_dir, transform=None):\n",
    "        self.damage_dir = damage_dir\n",
    "        self.origin_dir = origin_dir\n",
    "        self.transform = transform\n",
    "        self.damage_files = sorted(os.listdir(damage_dir))\n",
    "        self.origin_files = sorted(os.listdir(origin_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.damage_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        damage_img_name = self.damage_files[idx]\n",
    "        origin_img_name = self.origin_files[idx]\n",
    "\n",
    "        damage_img_path = os.path.join(self.damage_dir, damage_img_name)\n",
    "        origin_img_path = os.path.join(self.origin_dir, origin_img_name)\n",
    "\n",
    "        damage_img = Image.open(damage_img_path).convert(\"RGB\")\n",
    "        origin_img = Image.open(origin_img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            damage_img = self.transform(damage_img)\n",
    "            origin_img = self.transform(origin_img)\n",
    "\n",
    "        return {'A': damage_img, 'B': origin_img}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# 경로 설정\n",
    "origin_dir = 'data/train_gt'  # 원본 이미지 폴더 경로\n",
    "damage_dir = 'data/train_input'  # 손상된 이미지 폴더 경로\n",
    "test_dir = 'data/test_input'     # test 이미지 폴더 경로\n",
    "\n",
    "# 데이터 전처리 설정\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "# 전체 데이터셋 생성\n",
    "dataset = CustomDataset(damage_dir=damage_dir, origin_dir=origin_dir, transform=transform)\n",
    "\n",
    "# 데이터셋을 학습과 검증으로 나누기 (예: 80% 학습, 20% 검증)\n",
    "validation_ratio = 0.2\n",
    "train_size = int((1 - validation_ratio) * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "training_dataset, validation_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "\n",
    "# 1. gpt가 짜준 ssimscore랑 공모전 metric 코드랑 뭐가 다른지 비교\n",
    "\n",
    "# 2. bceloss랑 poerceptual loss 들어간 새로운 combined loss로 테스트해보기\n",
    "\n",
    "# # 편법\n",
    "# 3. 1)\n",
    "# train_sizeL 29603 \n",
    "# test: 100\n",
    "# 우리가 갖ㄱ도 있는 최고의의 모데리: 0.5\n",
    "# test=> 대충정답: 컬러사진\n",
    "\n",
    "# 2). 그다음학습 때\n",
    "# training_dataset =>   (0.8*dataset) + + test=> 대충정답: 컬러사진\n",
    "# validation_dataset: (0.2*dataset) + test=> 대충정답: 컬러사진\n",
    "\n",
    "\n",
    "# 학습 및 검증 DataLoader 설정\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=training_dataset,\n",
    "    batch_size=CFG['BATCH_SIZE'],\n",
    "    shuffle=True,\n",
    "    num_workers=0  # 멀티프로세싱 비활성화\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    dataset=validation_dataset, \n",
    "    batch_size=CFG['BATCH_SIZE'], \n",
    "    shuffle=False, \n",
    "    num_workers=8, \n",
    "    pin_memory=True, \n",
    "    persistent_workers=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PatchGAN 기반의 Discriminator\n",
    "class PatchGANDiscriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(PatchGANDiscriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, normalization=True):\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, kernel_size=4, stride=2, padding=1)]\n",
    "            if normalization:\n",
    "                layers.append(nn.BatchNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return nn.Sequential(*layers)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            discriminator_block(in_channels * 2, 64, normalization=False),\n",
    "            discriminator_block(64, 128),\n",
    "            discriminator_block(128, 256),\n",
    "            discriminator_block(256, 512),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, img_A, img_B):\n",
    "        img_input = torch.cat((img_A, img_B), 1)\n",
    "        return self.model(img_input)\n",
    "\n",
    "# 가중치 초기화 함수\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm2d') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation_models_pytorch import UnetPlusPlus\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "# EfficientNet-B4를 백본으로 사용\n",
    "UNetPP = UnetPlusPlus(\n",
    "    encoder_name=\"efficientnet-b4\",  # pretrained on ImageNet\n",
    "    encoder_weights=\"imagenet\",     \n",
    "    in_channels=3,\n",
    "    classes=3\n",
    ").to(device)\n",
    "\n",
    "# generator로 UNetPP를 사용하도록 설정\n",
    "generator = UNetPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/2] [Batch 0/1481] [D loss: 0.4553558826446533] [G loss: 76.28998565673828] [D accuracy: 49.06%]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from IPython.display import FileLink\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# 모델 저장을 위한 디렉토리 생성\n",
    "model_save_dir = \"./saved_models\"\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "log_file = os.path.join(model_save_dir, \"training_log.txt\")\n",
    "\n",
    "# Best score와 loss 초기화\n",
    "best_loss = float(\"inf\")\n",
    "best_score = float(\"-inf\")\n",
    "lambda_pixel = 100  # 픽셀 손실에 대한 가중치\n",
    "\n",
    "# PatchGAN Discriminator 초기화\n",
    "discriminator = PatchGANDiscriminator().to(device)\n",
    "\n",
    "# 손실 함수 및 옵티마이저 설정\n",
    "criterion_GAN = nn.MSELoss()\n",
    "criterion_pixelwise = nn.L1Loss()\n",
    "\n",
    "# Generator와 Discriminator 옵티마이저 설정\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=CFG[\"LEARNING_RATE\"])\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=CFG[\"LEARNING_RATE\"])\n",
    "\n",
    "# 색상 히스토그램 유사도 함수\n",
    "def histogram_similarity(pred, target, bins=256):\n",
    "    pred_hist = cv2.calcHist([pred.cpu().numpy()], [0], None, [bins], [0, 256])\n",
    "    target_hist = cv2.calcHist([target.cpu().numpy()], [0], None, [bins], [0, 256])\n",
    "    hist_similarity = cv2.compareHist(pred_hist, target_hist, cv2.HISTCMP_CORREL)\n",
    "    return hist_similarity\n",
    "\n",
    "# 손실 함수\n",
    "def combined_loss(fake_B, real_B):\n",
    "    loss_GAN = criterion_GAN(fake_B, real_B)\n",
    "    loss_pixel = criterion_pixelwise(fake_B, real_B)\n",
    "    loss_ssim = 1 - ssim(fake_B, real_B, data_range=1.0)\n",
    "    loss_hist = histogram_similarity(fake_B, real_B)\n",
    "    return loss_GAN + lambda_pixel * loss_pixel + 0.4 * loss_ssim + 0.4 * loss_hist\n",
    "\n",
    "def ssim_score(true, pred):\n",
    "    true_np = true.permute(0, 2, 3, 1).cpu().numpy()  # NCHW → NHWC\n",
    "    pred_np = pred.permute(0, 2, 3, 1).cpu().numpy()\n",
    "    ssim_values = []\n",
    "    for t, p in zip(true_np, pred_np):\n",
    "        ssim_values.append(ssim(t, p, channel_axis=-1, data_range=p.max() - p.min()))\n",
    "    return np.mean(ssim_values)\n",
    "\n",
    "def masked_ssim_score(true, pred, mask):\n",
    "    true_np = true.permute(0, 2, 3, 1).cpu().numpy()  # NCHW → NHWC\n",
    "    pred_np = pred.permute(0, 2, 3, 1).cpu().numpy()\n",
    "    mask_np = mask.cpu().numpy()  # 마스크를 NumPy 배열로 변환\n",
    "    masked_ssim_values = []\n",
    "    for t, p, m in zip(true_np, pred_np, mask_np):\n",
    "        true_masked_pixels = t[m > 0]\n",
    "        pred_masked_pixels = p[m > 0]\n",
    "        masked_ssim_values.append(\n",
    "            ssim(\n",
    "                true_masked_pixels, \n",
    "                pred_masked_pixels, \n",
    "                channel_axis=-1, \n",
    "                data_range=pred_masked_pixels.max() - pred_masked_pixels.min()\n",
    "            )\n",
    "        )\n",
    "    return np.mean(masked_ssim_values)\n",
    "\n",
    "def histogram_similarity(true, pred):\n",
    "    true_np = true.permute(0, 2, 3, 1).cpu().numpy().astype(np.uint8)  # NCHW → NHWC\n",
    "    pred_np = pred.permute(0, 2, 3, 1).cpu().numpy().astype(np.uint8)\n",
    "    similarities = []\n",
    "    for t, p in zip(true_np, pred_np):\n",
    "        true_hsv = cv2.cvtColor(t, cv2.COLOR_RGB2HSV)\n",
    "        pred_hsv = cv2.cvtColor(p, cv2.COLOR_RGB2HSV)\n",
    "        hist_true = cv2.calcHist([true_hsv], [0], None, [180], [0, 180])\n",
    "        hist_pred = cv2.calcHist([pred_hsv], [0], None, [180], [0, 180])\n",
    "        hist_true = cv2.normalize(hist_true, hist_true).flatten()\n",
    "        hist_pred = cv2.normalize(hist_pred, hist_pred).flatten()\n",
    "        similarities.append(cv2.compareHist(hist_true, hist_pred, cv2.HISTCMP_CORREL))\n",
    "    return np.mean(similarities)\n",
    "\n",
    "# 학습 및 검증 루프\n",
    "for epoch in range(1, CFG['EPOCHS'] + 1):\n",
    "    generator.train()  # 학습 모드\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        real_A = batch['A'].to(device)\n",
    "        real_B = batch['B'].to(device)\n",
    "\n",
    "        # Generator 훈련\n",
    "        optimizer_G.zero_grad()\n",
    "        fake_B = generator(real_A)\n",
    "        pred_fake = discriminator(fake_B, real_A)\n",
    "        loss_GAN = criterion_GAN(pred_fake, torch.ones_like(pred_fake).to(device))\n",
    "        loss_pixel = criterion_pixelwise(fake_B, real_B)\n",
    "        loss_G = loss_GAN + lambda_pixel * loss_pixel\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # Discriminator 훈련\n",
    "        optimizer_D.zero_grad()\n",
    "        pred_real = discriminator(real_B, real_A)\n",
    "        loss_real = criterion_GAN(pred_real, torch.ones_like(pred_real).to(device))\n",
    "        pred_fake = discriminator(fake_B.detach(), real_A)\n",
    "        loss_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake).to(device))\n",
    "        loss_D = 0.5 * (loss_real + loss_fake)\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Discriminator 정확도 계산\n",
    "        real_accuracy = torch.mean((pred_real > 0.5).float())\n",
    "        fake_accuracy = torch.mean((pred_fake < 0.5).float())\n",
    "        accuracy = 0.5 * (real_accuracy + fake_accuracy)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"[Epoch {epoch}/{CFG['EPOCHS']}] [Batch {i}/{len(train_dataloader)}] \"\n",
    "                  f\"[D loss: {loss_D.item()}] [G loss: {loss_G.item()}] [D accuracy: {accuracy.item() * 100:.2f}%]\")\n",
    "\n",
    "        # Validation 및 모델 저장\n",
    "        if (i + 1) % 10 == 0 or i == len(train_dataloader) - 1:  # 10배치마다 또는 마지막 배치마다 저장\n",
    "            try:\n",
    "                generator.eval()\n",
    "                validation_loss = 0.0\n",
    "                with torch.no_grad():\n",
    "                    for val_batch in validation_dataloader:\n",
    "                        real_val_A = val_batch['A'].to(device)\n",
    "                        real_val_B = val_batch['B'].to(device)\n",
    "                        fake_val_B = generator(real_val_A)\n",
    "                        val_loss = combined_loss(fake_val_B, real_val_B)\n",
    "                        validation_loss += val_loss.item()\n",
    "\n",
    "                validation_loss /= len(validation_dataloader)\n",
    "                print(f\"Validation loss at Epoch {epoch}, Batch {i + 1}: {validation_loss}\")\n",
    "\n",
    "                # 모델 저장\n",
    "                if validation_loss < best_loss:\n",
    "                    best_loss = validation_loss\n",
    "                    torch.save(generator.state_dict(), os.path.join(model_save_dir, f\"best_generator_epoch{epoch}_batch{i + 1}.pth\"))\n",
    "                    torch.save(discriminator.state_dict(), os.path.join(model_save_dir, f\"best_discriminator_epoch{epoch}_batch{i + 1}.pth\"))\n",
    "                    print(f\"Best model saved at Epoch {epoch}, Batch {i + 1} with Validation loss: {validation_loss}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error during validation or saving: {e}\")\n",
    "\n",
    "\n",
    "    # Validation 단계\n",
    "    generator.eval()  # 검증 모드\n",
    "    validation_loss = 0.0\n",
    "    ssim_scores, masked_ssim_scores, hist_similarities = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for val_batch in validation_dataloader:\n",
    "            real_val_A = val_batch['A'].to(device)  # 손실 영역 마스크\n",
    "            real_val_B = val_batch['B'].to(device)  # Ground Truth\n",
    "            fake_val_B = generator(real_val_A)      # 복원된 이미지\n",
    "    \n",
    "            # 각각의 스코어 계산\n",
    "            ssim_value = ssim_score(real_val_B, fake_val_B)\n",
    "            masked_ssim_value = masked_ssim_score(real_val_B, fake_val_B, real_val_A)\n",
    "            hist_value = histogram_similarity(real_val_B, fake_val_B)\n",
    "    \n",
    "            ssim_scores.append(ssim_value)\n",
    "            masked_ssim_scores.append(masked_ssim_value)\n",
    "            hist_similarities.append(hist_value)\n",
    "    \n",
    "            val_loss = combined_loss(fake_val_B, real_val_B)\n",
    "            validation_loss += val_loss.item()\n",
    "    \n",
    "    # 평균 스코어 계산\n",
    "    S = sum(ssim_scores) / len(ssim_scores)\n",
    "    M = sum(masked_ssim_scores) / len(masked_ssim_scores)\n",
    "    C = sum(hist_similarities) / len(hist_similarities)\n",
    "    score = (0.2 * S) + (0.4 * M) + (0.4 * C)\n",
    "    \n",
    "    validation_loss /= len(validation_dataloader)\n",
    "    validation_log_message = f\"Validation loss: {validation_loss}, Score: {score}\"\n",
    "    print(validation_log_message)\n",
    "    \n",
    "    # 텍스트 파일에 Validation 손실 기록\n",
    "    with open(log_file, \"a\") as log:\n",
    "        log.write(validation_log_message + \"\\n\")\n",
    "    \n",
    "    # SSIM 평가 기준에 따른 최적 모델 저장\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        torch.save(generator.state_dict(), os.path.join(model_save_dir, \"best_generator.pth\"))\n",
    "        torch.save(discriminator.state_dict(), os.path.join(model_save_dir, \"best_discriminator.pth\"))\n",
    "        save_message = f\"Best model saved at epoch {epoch} with Score: {score}\"\n",
    "        print(save_message)\n",
    "        with open(log_file, \"a\") as log:\n",
    "            log.write(save_message + \"\\n\")\n",
    "\n",
    "# 학습 완료 후 최고의 스코어 출력\n",
    "final_message = f\"Training completed. Best Score: {best_score}\"\n",
    "print(final_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T09:11:36.631338Z",
     "iopub.status.busy": "2024-11-18T09:11:36.630923Z",
     "iopub.status.idle": "2024-11-18T09:11:47.123872Z",
     "shell.execute_reply": "2024-11-18T09:11:47.122821Z",
     "shell.execute_reply.started": "2024-11-18T09:11:36.631300Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 저장할 디렉토리 설정\n",
    "model_save_dir = \"./saved_models\"\n",
    "submission_dir = \"./submission\"\n",
    "os.makedirs(submission_dir, exist_ok=True)\n",
    "\n",
    "# 이미지 로드 및 전처리\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image)\n",
    "    image = image.unsqueeze(0)  # 배치 차원을 추가합니다.\n",
    "    return image\n",
    "\n",
    "# 모델 경로 설정\n",
    "generator_path = os.path.join(\"/kaggle/input/best-model01/best_generator (1).pth\")\n",
    "\n",
    "# 모델 로드 및 설정\n",
    "# model = UNetPP(in_channels=3, out_channels=3).to(device)  # UNetPP로 설정\n",
    "UNetPP.load_state_dict(torch.load(generator_path))\n",
    "UNetPP.eval()\n",
    "\n",
    "# 파일 리스트 불러오기\n",
    "test_images = sorted(os.listdir(test_dir))\n",
    "\n",
    "# 모든 테스트 이미지에 대해 추론 수행\n",
    "for image_name in test_images:\n",
    "    test_image_path = os.path.join(test_dir, image_name)\n",
    "\n",
    "    # 손상된 테스트 이미지 로드 및 전처리\n",
    "    test_image = load_image(test_image_path).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 모델로 예측\n",
    "        pred_image = UNetPP(test_image)\n",
    "        pred_image = pred_image.cpu().squeeze(0)  # 배치 차원 제거\n",
    "        pred_image = pred_image * 0.5 + 0.5  # 역정규화\n",
    "        pred_image = pred_image.numpy().transpose(1, 2, 0)  # HWC로 변경\n",
    "        pred_image = (pred_image * 255).astype('uint8')  # 0-255 범위로 변환\n",
    "        \n",
    "        # 예측된 이미지를 실제 이미지와 같은 512x512로 리사이즈\n",
    "        pred_image_resized = cv2.resize(pred_image, (512, 512), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # 결과 이미지 저장\n",
    "    output_path = os.path.join(submission_dir, image_name)\n",
    "    cv2.imwrite(output_path, cv2.cvtColor(pred_image_resized, cv2.COLOR_RGB2BGR))    \n",
    "    \n",
    "print(f\"Saved all images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T09:11:51.006020Z",
     "iopub.status.busy": "2024-11-18T09:11:51.005310Z",
     "iopub.status.idle": "2024-11-18T09:11:51.130683Z",
     "shell.execute_reply": "2024-11-18T09:11:51.129745Z",
     "shell.execute_reply.started": "2024-11-18T09:11:51.005970Z"
    }
   },
   "outputs": [],
   "source": [
    "# 저장된 결과 이미지를 ZIP 파일로 압축\n",
    "zip_filename = \"submission1.zip\"\n",
    "with zipfile.ZipFile(zip_filename, 'w') as submission_zip:\n",
    "    for image_name in test_images:\n",
    "        image_path = os.path.join(submission_dir, image_name)\n",
    "        submission_zip.write(image_path, arcname=image_name)\n",
    "\n",
    "print(f\"All images saved in {zip_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T02:15:56.140763Z",
     "iopub.status.busy": "2024-11-15T02:15:56.140088Z",
     "iopub.status.idle": "2024-11-15T02:15:57.214677Z",
     "shell.execute_reply": "2024-11-15T02:15:57.213429Z",
     "shell.execute_reply.started": "2024-11-15T02:15:56.140720Z"
    }
   },
   "outputs": [],
   "source": [
    "!tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6075062,
     "sourceId": 9891694,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6078662,
     "sourceId": 9896364,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6111024,
     "sourceId": 9939640,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
