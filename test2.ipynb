{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import cv2\n",
    "\n",
    "import zipfile\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(\"Using device:\", device)\n",
    "\n",
    "# print(torch.cuda.is_available())  # True인지 확인\n",
    "# print(torch.cuda.device_count())  # 사용 가능한 GPU 개수 확인\n",
    "# print(torch.cuda.current_device())  # 현재 사용 중인 GPU 인덱스 확인\n",
    "# print(torch.cuda.get_device_name(0))  # 사용 중인 GPU 이름 확인\n",
    "# print(torch.version.cuda)  # PyTorch가 사용하는 CUDA 버전 출력\n",
    "\n",
    "# origin_dir = 'train_gt'\n",
    "# damage_dir = 'train_input'\n",
    "\n",
    "# print(len(os.listdir(origin_dir)))\n",
    "# print(len(os.listdir(damage_dir)))\n",
    "\n",
    "# seed_everything(42)\n",
    "\n",
    "# # 테스트 난수\n",
    "# print(\"Random:\", random.random())\n",
    "# print(\"NumPy Random:\", np.random.rand(1))\n",
    "# print(\"PyTorch Random:\", torch.rand(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'EPOCHS':2,\n",
    "    'LEARNING_RATE':3e-4,\n",
    "    # 'BATCH_SIZE':16,\n",
    "    'BATCH_SIZE':8,\n",
    "    'SEED':42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def seed_everything(seed):\n",
    "#     random.seed(seed)\n",
    "#     os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed(seed)\n",
    "#     torch.backends.cudnn.deterministic = True  # 결정론적 연산 보장\n",
    "#     torch.backends.cudnn.benchmark = False     # 성능 최적화 대신 일관성 우선\n",
    "\n",
    "# seed_everything(CFG['SEED'])  # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_image(damage_img_path, origin_img_path):\n",
    "    # OpenCV로 이미지 읽기 (NumPy 배열로 읽음)\n",
    "    color_image = cv2.imread(origin_img_path)\n",
    "    gray_image = cv2.imread(damage_img_path, cv2.IMREAD_GRAYSCALE)  # 흑백 이미지로 읽기\n",
    "    \n",
    "    # 색상 이미지를 흑백으로 변환 (PIL로 변환 후 NumPy로 변환)\n",
    "    color_image_gray = cv2.cvtColor(color_image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # 두 이미지의 차이 계산\n",
    "    difference = cv2.absdiff(color_image_gray, gray_image)\n",
    "    \n",
    "    # 차이 값을 임계값으로 처리하여 이진화 이미지 생성\n",
    "    _, binary_difference = cv2.threshold(difference, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # 마스크 생성\n",
    "    mask = binary_difference > 0  # 차이가 있는 부분을 마스크로 설정\n",
    "    mask = Image.fromarray(mask.astype(np.uint8) * 255)  # 마스크 이미지를 PIL 형식으로 변환\n",
    "\n",
    "    return {\n",
    "        'image_gray_masked': Image.fromarray(gray_image),  # 손상된 이미지를 PIL 이미지로 반환\n",
    "        'mask': transforms.ToTensor()(mask)  # 마스크를 텐서로 변환하여 사용\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, damage_dir, origin_dir, transform=None, use_masks=True):\n",
    "        self.damage_dir = damage_dir\n",
    "        self.origin_dir = origin_dir\n",
    "        self.transform = transform\n",
    "        self.use_masks = use_masks\n",
    "        self.damage_files = sorted(os.listdir(damage_dir), key=lambda x: x.lower())\n",
    "        self.origin_files = sorted(os.listdir(origin_dir), key=lambda x: x.lower())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.damage_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        damage_img_name = self.damage_files[idx]\n",
    "        origin_img_name = self.origin_files[idx]\n",
    "\n",
    "        damage_img_path = os.path.join(self.damage_dir, damage_img_name)\n",
    "        origin_img_path = os.path.join(self.origin_dir, origin_img_name)\n",
    "\n",
    "        damage_img = Image.open(damage_img_path).convert(\"RGB\")\n",
    "        origin_img = Image.open(origin_img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.use_masks:\n",
    "            input_data = get_input_image(damage_img_path, origin_img_path)\n",
    "            mask = input_data['mask']\n",
    "            # `mask`가 이미 텐서인지 확인하고 변환 처리\n",
    "            if not isinstance(mask, torch.Tensor):\n",
    "                mask = transforms.ToTensor()(mask)\n",
    "        else:\n",
    "            mask = torch.zeros((1, damage_img.size[1], damage_img.size[0]))\n",
    "\n",
    "        if self.transform:\n",
    "            damage_img = self.transform(damage_img)\n",
    "            origin_img = self.transform(origin_img)\n",
    "\n",
    "        return {'A': damage_img, 'B': origin_img, 'mask': mask}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zqrc0\\anaconda3\\envs\\new_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b0-355c32eb.pth\" to C:\\Users\\zqrc0/.cache\\torch\\hub\\checkpoints\\efficientnet-b0-355c32eb.pth\n",
      "100%|██████████| 20.4M/20.4M [00:01<00:00, 10.9MB/s]\n"
     ]
    }
   ],
   "source": [
    "from segmentation_models_pytorch import UnetPlusPlus\n",
    "import torch.nn.functional as F\n",
    "import lightning as L\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 히스토그램 유사도 계산 함수\n",
    "def get_histogram_similarity(true_np, pred_np, color_space=cv2.COLOR_RGB2HSV):\n",
    "    true_hsv = cv2.cvtColor(true_np.astype(np.uint8), color_space)\n",
    "    pred_hsv = cv2.cvtColor(pred_np.astype(np.uint8), color_space)\n",
    "    \n",
    "    hist_true = cv2.calcHist([true_hsv], [0], None, [180], [0, 180])\n",
    "    hist_pred = cv2.calcHist([pred_hsv], [0], None, [180], [0, 180])\n",
    "    \n",
    "    hist_true = cv2.normalize(hist_true, hist_true).flatten()\n",
    "    hist_pred = cv2.normalize(hist_pred, hist_pred).flatten()\n",
    "    \n",
    "    similarity = cv2.compareHist(hist_true, hist_pred, cv2.HISTCMP_CORREL)\n",
    "    return similarity\n",
    "\n",
    "# 마스크된 부분만 SSIM 계산\n",
    "def get_masked_ssim_score(true_np, pred_np, mask_np):\n",
    "    if mask_np.ndim == 3 and mask_np.shape[0] == 1:\n",
    "        mask_np = mask_np.squeeze(0)\n",
    "    elif mask_np.ndim == 3 and mask_np.shape[-1] == 1:\n",
    "        mask_np = mask_np.squeeze(-1)\n",
    "    \n",
    "    if true_np.shape[:2] != mask_np.shape:\n",
    "        mask_np = cv2.resize(mask_np, (true_np.shape[1], true_np.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    true_masked = true_np[mask_np > 0]\n",
    "    pred_masked = pred_np[mask_np > 0]\n",
    "\n",
    "    if true_masked.size == 0 or pred_masked.size == 0:\n",
    "        return 0\n",
    "\n",
    "    ssim_value = ssim(\n",
    "        true_masked, pred_masked, data_range=pred_masked.max() - pred_masked.min(), channel_axis=-1\n",
    "    )\n",
    "    return ssim_value\n",
    "\n",
    "# SSIM 계산 함수\n",
    "def get_ssim_score(true_np, pred_np):\n",
    "    ssim_value = ssim(\n",
    "        true_np, pred_np, channel_axis=-1, data_range=pred_np.max() - pred_np.min()\n",
    "    )\n",
    "    if np.isnan(ssim_value):\n",
    "        ssim_value = 0\n",
    "    return ssim_value\n",
    "\n",
    "# Lightning Module 정의\n",
    "class LitIRModel(L.LightningModule):\n",
    "    def __init__(self, model_1, model_2, image_mean=0.5, image_std=0.5):\n",
    "        super().__init__()\n",
    "        self.model_1 = model_1\n",
    "        self.model_2 = model_2\n",
    "        self.image_mean = image_mean\n",
    "        self.image_std = image_std\n",
    "\n",
    "    def forward(self, images_gray_masked):\n",
    "        images_gray_restored = self.model_1(images_gray_masked) + images_gray_masked\n",
    "        images_restored = self.model_2(images_gray_restored)\n",
    "        return images_gray_restored, images_restored\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.AdamW(self.parameters(), lr=1e-5)\n",
    "        return opt\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images_gray_masked = torch.mean(batch['A'], dim=1, keepdim=True)  # 손상된 이미지를 흑백으로 변환\n",
    "        images_gt = batch['B']  # Ground Truth 이미지\n",
    "\n",
    "        # 모델에 입력\n",
    "        images_gray_restored, images_restored = self(images_gray_masked)\n",
    "    \n",
    "        # 손실 계산\n",
    "        loss_pixel_gray = (\n",
    "            F.l1_loss(images_gray_masked, images_gray_restored, reduction='mean') * 0.5 +\n",
    "            F.mse_loss(images_gray_masked, images_gray_restored, reduction='mean') * 0.5\n",
    "        )\n",
    "        loss_pixel = (\n",
    "            F.l1_loss(images_gt, images_restored, reduction='mean') * 0.5 +\n",
    "            F.mse_loss(images_gt, images_restored, reduction='mean') * 0.5\n",
    "        )\n",
    "        loss = loss_pixel_gray * 0.5 + loss_pixel * 0.5\n",
    "\n",
    "        # 로깅 (Batch와 손실 값 출력)\n",
    "        print(f\"Batch {batch_idx}, Loss: {loss.item()}\")\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images_gray_masked = torch.mean(batch['A'], dim=1, keepdim=True)  # 손상된 이미지를 흑백으로 변환\n",
    "        images_gt = batch['B']  # Ground Truth 이미지\n",
    "\n",
    "        images_gray_restored, images_restored = self(images_gray_masked)\n",
    "\n",
    "        # Ground Truth와 복원된 이미지 크기 맞추기\n",
    "        if images_restored.shape != images_gt.shape:\n",
    "            images_restored = torch.nn.functional.interpolate(\n",
    "                images_restored, size=images_gt.shape[-2:], mode=\"bilinear\", align_corners=False\n",
    "            )\n",
    "\n",
    "        # NumPy 변환\n",
    "        images_restored_np = images_restored.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "        images_gt_np = images_gt.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "\n",
    "        # SSIM 및 기타 메트릭 계산\n",
    "        total_ssim_score = get_ssim_score(images_gt_np[0], images_restored_np[0])\n",
    "        self.log(\"val_ssim\", total_ssim_score, on_step=False, on_epoch=True)\n",
    "        return {\"val_ssim\": total_ssim_score}\n",
    "\n",
    "# 모델 초기화\n",
    "model_1 = UnetPlusPlus(\n",
    "    encoder_name=\"efficientnet-b0\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=1,  # Grayscale 입력\n",
    "    classes=1\n",
    ")\n",
    "\n",
    "model_2 = UnetPlusPlus(\n",
    "    encoder_name=\"efficientnet-b0\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=1,  # Grayscale 입력\n",
    "    classes=3  # RGB 출력\n",
    ")\n",
    "\n",
    "lit_ir_model = LitIRModel(model_1=model_1, model_2=model_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: output/04/TEST_000.png\n",
      "Saved: output/04/TEST_001.png\n",
      "Saved: output/04/TEST_002.png\n",
      "Saved: output/04/TEST_003.png\n",
      "Saved: output/04/TEST_004.png\n",
      "Saved: output/04/TEST_005.png\n",
      "Saved: output/04/TEST_006.png\n",
      "Saved: output/04/TEST_007.png\n",
      "Saved: output/04/TEST_008.png\n",
      "Saved: output/04/TEST_009.png\n",
      "Saved: output/04/TEST_010.png\n",
      "Saved: output/04/TEST_011.png\n",
      "Saved: output/04/TEST_012.png\n",
      "Saved: output/04/TEST_013.png\n",
      "Saved: output/04/TEST_014.png\n",
      "Saved: output/04/TEST_015.png\n",
      "Saved: output/04/TEST_016.png\n",
      "Saved: output/04/TEST_017.png\n",
      "Saved: output/04/TEST_018.png\n",
      "Saved: output/04/TEST_019.png\n",
      "Saved: output/04/TEST_020.png\n",
      "Saved: output/04/TEST_021.png\n",
      "Saved: output/04/TEST_022.png\n",
      "Saved: output/04/TEST_023.png\n",
      "Saved: output/04/TEST_024.png\n",
      "Saved: output/04/TEST_025.png\n",
      "Saved: output/04/TEST_026.png\n",
      "Saved: output/04/TEST_027.png\n",
      "Saved: output/04/TEST_028.png\n",
      "Saved: output/04/TEST_029.png\n",
      "Saved: output/04/TEST_030.png\n",
      "Saved: output/04/TEST_031.png\n",
      "Saved: output/04/TEST_032.png\n",
      "Saved: output/04/TEST_033.png\n",
      "Saved: output/04/TEST_034.png\n",
      "Saved: output/04/TEST_035.png\n",
      "Saved: output/04/TEST_036.png\n",
      "Saved: output/04/TEST_037.png\n",
      "Saved: output/04/TEST_038.png\n",
      "Saved: output/04/TEST_039.png\n",
      "Saved: output/04/TEST_040.png\n",
      "Saved: output/04/TEST_041.png\n",
      "Saved: output/04/TEST_042.png\n",
      "Saved: output/04/TEST_043.png\n",
      "Saved: output/04/TEST_044.png\n",
      "Saved: output/04/TEST_045.png\n",
      "Saved: output/04/TEST_046.png\n",
      "Saved: output/04/TEST_047.png\n",
      "Saved: output/04/TEST_048.png\n",
      "Saved: output/04/TEST_049.png\n",
      "Saved: output/04/TEST_050.png\n",
      "Saved: output/04/TEST_051.png\n",
      "Saved: output/04/TEST_052.png\n",
      "Saved: output/04/TEST_053.png\n",
      "Saved: output/04/TEST_054.png\n",
      "Saved: output/04/TEST_055.png\n",
      "Saved: output/04/TEST_056.png\n",
      "Saved: output/04/TEST_057.png\n",
      "Saved: output/04/TEST_058.png\n",
      "Saved: output/04/TEST_059.png\n",
      "Saved: output/04/TEST_060.png\n",
      "Saved: output/04/TEST_061.png\n",
      "Saved: output/04/TEST_062.png\n",
      "Saved: output/04/TEST_063.png\n",
      "Saved: output/04/TEST_064.png\n",
      "Saved: output/04/TEST_065.png\n",
      "Saved: output/04/TEST_066.png\n",
      "Saved: output/04/TEST_067.png\n",
      "Saved: output/04/TEST_068.png\n",
      "Saved: output/04/TEST_069.png\n",
      "Saved: output/04/TEST_070.png\n",
      "Saved: output/04/TEST_071.png\n",
      "Saved: output/04/TEST_072.png\n",
      "Saved: output/04/TEST_073.png\n",
      "Saved: output/04/TEST_074.png\n",
      "Saved: output/04/TEST_075.png\n",
      "Saved: output/04/TEST_076.png\n",
      "Saved: output/04/TEST_077.png\n",
      "Saved: output/04/TEST_078.png\n",
      "Saved: output/04/TEST_079.png\n",
      "Saved: output/04/TEST_080.png\n",
      "Saved: output/04/TEST_081.png\n",
      "Saved: output/04/TEST_082.png\n",
      "Saved: output/04/TEST_083.png\n",
      "Saved: output/04/TEST_084.png\n",
      "Saved: output/04/TEST_085.png\n",
      "Saved: output/04/TEST_086.png\n",
      "Saved: output/04/TEST_087.png\n",
      "Saved: output/04/TEST_088.png\n",
      "Saved: output/04/TEST_089.png\n",
      "Saved: output/04/TEST_090.png\n",
      "Saved: output/04/TEST_091.png\n",
      "Saved: output/04/TEST_092.png\n",
      "Saved: output/04/TEST_093.png\n",
      "Saved: output/04/TEST_094.png\n",
      "Saved: output/04/TEST_095.png\n",
      "Saved: output/04/TEST_096.png\n",
      "Saved: output/04/TEST_097.png\n",
      "Saved: output/04/TEST_098.png\n",
      "Saved: output/04/TEST_099.png\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "output_dir = \"output/04/\"\n",
    "model_path = 'saved_models/112302best_model-epoch=00-val_ssim=0.5549.ckpt'\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, damage_dir, origin_dir, transform=None, use_masks=True):\n",
    "        self.damage_dir = damage_dir\n",
    "        self.origin_dir = origin_dir\n",
    "        self.transform = transform\n",
    "        self.use_masks = use_masks\n",
    "        self.damage_files = sorted(os.listdir(damage_dir), key=lambda x: x.lower())\n",
    "        self.origin_files = sorted(os.listdir(origin_dir), key=lambda x: x.lower())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.damage_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        damage_img_name = self.damage_files[idx]\n",
    "        origin_img_name = self.origin_files[idx]\n",
    "\n",
    "        damage_img_path = os.path.join(self.damage_dir, damage_img_name)\n",
    "        origin_img_path = os.path.join(self.origin_dir, origin_img_name)\n",
    "\n",
    "        damage_img = Image.open(damage_img_path).convert(\"RGB\")\n",
    "        origin_img = Image.open(origin_img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.use_masks:\n",
    "            input_data = get_input_image(damage_img_path, origin_img_path)\n",
    "            mask = input_data['mask']\n",
    "            # `mask`가 이미 텐서인지 확인하고 변환 처리\n",
    "            if not isinstance(mask, torch.Tensor):\n",
    "                mask = transforms.ToTensor()(mask)\n",
    "        else:\n",
    "            mask = torch.zeros((1, damage_img.size[1], damage_img.size[0]))\n",
    "\n",
    "        if self.transform:\n",
    "            damage_img = self.transform(damage_img)\n",
    "            origin_img = self.transform(origin_img)\n",
    "\n",
    "        return {'A': damage_img, 'B': origin_img, 'mask': mask, 'filename': damage_img_name}\n",
    "\n",
    "\n",
    "# 데이터 전처리\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "test_dir = 'data/test_input'\n",
    "\n",
    "# 테스트 데이터셋 생성\n",
    "test_dataset = CustomDataset(damage_dir=test_dir, origin_dir=test_dir, transform=transform, use_masks=False)\n",
    "\n",
    "# 테스트 DataLoader 생성\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,  # 배치 크기를 1로 설정하여 각 파일을 개별적으로 처리\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# 모델 초기화\n",
    "model = LitIRModel.load_from_checkpoint(\n",
    "    checkpoint_path=model_path,\n",
    "    model_1=model_1,\n",
    "    model_2=model_2\n",
    ")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 모델을 평가 모드로 설정\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# 테스트 데이터로 예측 실행\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "model.eval()  # 모델을 평가 모드로 설정\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, batch in enumerate(test_dataloader):\n",
    "        # 입력 데이터 준비 (RGB -> Grayscale 변환)\n",
    "        inputs = torch.mean(batch['A'], dim=1, keepdim=True).to(device)  # [N, 1, H, W]\n",
    "        \n",
    "        # 모델 예측\n",
    "        gray_restored, color_restored = model(inputs)  # 모델 예측\n",
    "\n",
    "        # 파일명 가져오기\n",
    "        filename = batch['filename'][0]  # 배치 크기가 1이므로 첫 번째 파일명만 사용\n",
    "\n",
    "        # 예측된 이미지를 저장\n",
    "        for i, result in enumerate(color_restored):\n",
    "            result_img = (result.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)  # [C, H, W] -> [H, W, C]\n",
    "            output_path = os.path.join(output_dir, filename)  # 원본 파일명과 동일하게 저장\n",
    "            plt.imsave(output_path, result_img)\n",
    "            print(f\"Saved: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
